==============
General
==============

Storage
  - Architecture
    - Direct
      - on the physical host
      - fast but ephemeral, goes away when instance moves host, fails, etc.
      - instance store
    - Network
      - highly resilient, survives if host fails, persistent
      - EBS
  - Structure
    - Block Storage
      - Presented to OS as collection of blocks, no structure
        - just a colleciton of uniquely addressable blocks
      - file system is created on top of it
      - mountable, bootable
        - most EC2 instances boot from one
    - File Storage
      - presented as file share
      - file system is already built in
      - not bootable, OS only has high level access to files, not low level access to raw data
    - Object Storage
      - abstract
      - has metadata
      - flat key/object
      - scales well
      - not mountable or bootable
  - Performance
    - IO (block) size
    - IOps
    - Throughput: IO * IOps
      - unless there's a throughput cap
      - sometimes changing IO changes the IOps also

Virtualization
  - running more than one operating system at a time on a piece of hardware
  - Apps can't directly access hardware, only OS can because it runs in privileged mode, not user mode
    - but, only one thing could historically be privileged
      - 2 early solutions, had to be done in software
        - Emulated virtualization
          - guest OSs run in virtual machine
            - has allocation of cpu, etc.
          - host OS had a "hypervisor"
            - what the host thought was the hardware was emulated
            - hardware calls are sent to the hypervisor, which interfaces with the real hardware using binary translation
          - performance is slow
        - Paravirtualization
          - still has hypervisor
          - virtual machines run modified OSs (probably linux)
            - know about and make calls directly to the hypervisor instead of having hypervisor translate fake hardware calls
      - modern solution: Hardware Assisted Virtualization
        - CPU knows about virtualization, traps hardware instructions and does them
          - faster because CPU just does it faster
            - it has special circuits and stuff
          - Hypervisor still manages, but doesn't do the nitty gritty
            - sets up VMs, allocates resources, handles edge cases that CPU can't
      - moderner solution: Single Route IO Virtualization (SR-IOV)
        - hardware is virtualization-aware
          - a network card, say, can split itself into many mini cards, and those can be allocated to guest OSs with no translation
        - in EC2, this is called "Enhanced Networking"

Containers
  - virtualization still has issues
    - OS is duplicated and can take up a bunch of resources
      - a waste if multiple virtual machines use same OS (which is common)
  - Instead of hypervisor and multiple OSs, there is a host OS and Container Engine
  - instead of VM, we use container
    - still isolated, has its own file system
    - can run child processes inside
  - Portable and consistent, can run wherever there's a compatible host OS
  - can expose ports
  - Docker is the most popular container engine
    - Starts with Dockerfile that creates image
      - each line creates new file system layer in the createdimage
        - each layer is stored as diff
        - layers are read only
      - first line is base image to extend, or from "scratch" (literal string FROM scratch)
      - entrypoint is what initially runs
      - last layer is special read/write layer where stuff happens during container running
      - sharing the read-only layers is fast and cool
  - Container registry
    - can upload and download container images
    - can be public or private

Distributed Denial Of Service Attacks
  - 3 kinds
    - Network Volumetric
      - Layer 3
      - Saturate Capactiy with raw network data
    - Network Protocol (Transport)
      - Layer 4
      - example: TCP SYN Flood
      - leave a bunch of connections open
      - does not affect RAM, CPU, etc. but new connections can't be made
      - could be combined with volumetric
    - Application Layer
      - Layer 7
      - example: web request flood
        - site.com?search=hugeresponse

Firewalls
  - 3 kinds
    - Stateless (layer 3/4)
      - focuses on inbound and outbound
      - for https, say, client uses random port to connect to server's known 443 port.
        - "connection" is 2 requests: request and response
        - so, 2 rules needed: inbound and outbound, inverse of each other
          - server probably has to allow inbound traffic from all random ports on 443
          - from client's perspective, request is outgoing and response is incoming
          - vice versa for server
    - Stateful (layer 5)
      - focuses on request and response
      - smart enough to know that if request is allowed, response should be
        - knows that a given response is related to request, so only request rule is necessary
        - automatically opens response port when it makes a request
    - Application (layer 7)
      - understands higher layer things like http and other protocols, content
      - can scan for protocol-specific attacks, other unwanted traffic
        - https connection might terminate at firewall, stripping encryption so it can be scanned
        - can disallow facebook or dropbox, for instance

Shared Responsibility Model
  - what does user control vs AWS
    - On Premises -> Datacenter Hosted -> Infrastructure as a Service -> Platform as a Service -> Software as a Service
  - User is responsible for security ON the cloud
    - client-side stuff, operating system, firewall, file system, data, encryption, etc.
  - AWS is responsible for security OF the cloud
    - AWS software, infrastructure, storage, etc.

Availability
  - High Availability
    - Ensure performance for higher than normal period
      - period can be big or bigger
      - If something fails, it can quickly be replaced
      - replacement might make users log in again, for instance, but that's OK.

  - Fault Tolerance
    - ensure system keeps working through failure
      - Like if above example didn't make user log in again. Redundancy is a good tool here.

  - Disaster Recovery
    - What to do if HA/FT fail
    - pre-planned and documented processes
    - backups stored elsewhere from main storage

Public vs Private Services
  - Refers to networking and where you can access from
  - Private services can't be accessed from outside VPC (not connected to VPC)
  - Private Zone is like a home network, things inside can talk to stuff that things outside can't
    - VPCs are inside Private Zone
  - "AWS Public" zone is between public internet and VPC
    - Not inside public internet, but connected to it.
    - S3 for instance is here
    - You use public internet to get to AWS Public zone, which routes your requests internally
  - VPN can connect private-to-private network directly to private zone (edge case, eg on premises)
  - Internet Gateway can allow private zone stuff to access internet, or go directly to AWS Public zone for s3 queries, etc.
    - EC2 instance can have a public IP for instance.
      - Projects EC2 into Amazon Public Zone

Global Infrastructure
  - Regions -> Edge Locations
    - Edge locations are good for netflix, for instance. Better performance.
  - Regions give
    - Isolated fault domain
    - Geopolitical / legal separation
      - Different laws might apply
    - control; you might expand infra into a different region for better performance
  - Availability Zones
    - isolated hardware
    - architecture can distribute between them for region resiliance

Horizontal vs Vertical Scaling
  - Vertical
    - just use a bigger instance, dummy
    - max instance size is a limit
    - expensive
    - requires reboot (disruptive)
    - easy, no app changes needed
    - monoliths work
  - Horizontal
    - cheaper probably
    - no limit
    - granular, can add just a tiny bit more if you want
    - session management requires
      - application support
      - or off-host sessions
        - instances are stateless

Event driven architecture
  - doesn't constantly wait for things
    - fleet of lambda runners is already up and waiting for ANYTHING
  - microservices can scale up and down
  - producers make events, deliver them through event bus to consumers that do work and stop

Serverless architecture
  - use as little code as possible
  - third party services (like google auth + cognito)

Domain Registrar
  - Has relationships with major domain registries, which manage top level domains as vested by IANA
  - Registration process:
    - First, check with top level organization if domain is available
    - Create zonefile (database for domain with DNS info)
    - Allocate nameservers (4 per zone) - this is a "hosted zone"
      - Tell top level org about these. They add nameserver records that communicate with hosted zone

DNS Nameservers
  - authorative for their domain
  - stores these types of "resource records":
    - nameserver records
      - "amazon.com" in the .com zone points to servers in the amazon.com zone
      - root zone similarly points to .com zone
    - A & AAAA records
      - maps a name to an IP address
      - A maps to ipv4, AAAA maps to ipv6
        - usually create both for client compatibility
    - CNAME
      - Points a name to another name
        - could point www.blah.com to blah.com for instance
      - can't point apex (blah.com) at another name. Route53 has ALIAS record to do this
    - MX
      - routes emails to servers
        - A record for "mail"
        - MX record for "mail"
        - another MX record for "mail.stuff."
          - dot on right means it's fully qualified (not relative to domain)
            - can point outside zone, maybe at a different mail service that the zone owner likes better
        - mail server sends to: stuff@gmail.com
          - queries gmail.com for MX records
          - resolves priority (lower # is higher prio) 
          - gets hostname, then has to query for IP
    - TXT
      - arbitrary, can be used to prove to google for instance that you own the domain you say you do
        - other patterns exist, but the key is only the owner can add stuff and that helps
  - TTL
    - Walking the above tree takes time, but you get an authoritative answer for what IP to hit
    - This answer can come with a TTL, which allows resolver server (like 8.8.8.8, for instance) to cache answer (now non-authoritative)

Databases
  - Relational DataBase Management System
    - term incorrectly used interchangeably with Structured Query Language
      - SQL is the language used to store, update, retrieve data
      - RDBMS is the system, not the language
    - Data Structure
      - uses rigid schema
        - names of things, types, valid values, storage metadata
        - hard to change after data comes in
      - fixed relationships between tables
      - table structure
        - columns are attributes
        - rows are entries
        - cells are values
          - every cell must be populatd
      - Join Tables
        - defined in advance
        - imagine table with human 1 mapped to animal 1, human 1 mapped to animal 2, and human 2 mapped to animal 1
          - this is a human_animal table
          - composite keys (must be unique) are 1-1, 1-2 and 2-1
          - many to many
            - animal one belongs to humans 1 and 2
            - human one owns animals 1 and 2
  - Nonrelational (NoSQL)
    - more relaxed schemas
    - everything else, there's different models
      - Key-Value
        - no structure
        - fast, scalable
        - for simple requirements or in-memory caching
        - S3
      - Wide Column Store
        - partition key (think hashing bucket) for subset of table (whole data structure)
        - optionally, each row can have a clustering key
          - determine ordering of rows and physical layout on disk
        - items can have different columns
        - fast, scalable
        - DynamoDB
      - Document Database
        - Documents have JSON or whatever structure
          - structure can usually be different betwen items
          - documents have keys at least
        - like key-value store but DB is aware of contents
        - good for deep attribute interactions, heirarchical structure, whole-document operations
      - Row based DB
        - Online Transaction Processing db
        - like above, adding rows and working with rows is the main thing
      - Column based DB
        - inefficient for operating on rows
        - good for reporting, when all of a certain column will be needed
          - "which variant (where variant is a column) is sold the most?"
      - Graph based DB
        - relationships stored with data instead of being calculatd on query
        - each row might be a node (Person node and Company node, say)
        - edges between nodes have a name and a direction
          - John "works for" -> Amazon
          - edges can have key-value pairs like "startdate"
        - good for "who works for Amazon" queries because edges are stored with data
        - social media, complex relationships
  - Transaction Models
    - CAP Theorem - choose 2:
      - Consistency, every read will get latest write (or error)
      - Availability, every request will get non-error, without recency guarantee
      - Partition Tolerant, system functions even if there are communication errors between nodes
    - 2 models with tradeoffs
      - ACID: focuses on Consistency
        - limits scalability
        - A.tomic: all or none of the parts of transaction succeed, no partials
        - C.onsistent: DB moves from valid state to valid state, no in-between
        - I.solated: concurrent transactions behave the same as if they were sequential, no interference
        - D.urability: once transaction is reported successful, data has been written to durable storage
        - RDS tends to be ACID based
      - BASE: focuses on Availability
        - B.asically A.vailable: Does its best to read and write, but no guarantee of getting latest write
        - S.oft State: What you read might not have latest changes
          - writes might happen on another server or something
          - application has to deal with it
            - Might be special method to call to wait for "catch up"
        - E.ventual Consistency: even though latest things might take a bit, system will eventually be correct
        - NoSQL tends to be this
        - DynamoDB's basic operation, but can work acid-like with more advanced features

Encryption
  - asymmetric: private key decrypts, public key encrypts
  - symmetric: one key for encrypts and decrypts
  - the aim of most internet encryption protocols is to use slow asymmetric encryption to securely establish faster symmetric encryption

HTTPS(ecure)
  - http is simple and insecure
  - https is a layer to wrap http
  - in-transit encryption
  - certificates prove identity (you aren't at a spoofed DNS)
    - they are signed by an authority the browser trusts. DNS name is tied to certificate
    - certificate is used to generate a session key that a man in the middle couldn't know
      - this is generated via client-generated random data (pre-master) that is encrypted with the public key (it doesn't help mitm decrypt stuff or see session key)
      - private key can decrypt the pre-master and both can use additional randomness to generate the same session key

IPSEC
  - group of protocols to create tunnels in insecure networks
  - authenticatd and encrypted
  - concept of "Interesting Traffic"
    - if no interesting traffic, tunnels are torn down until more IT is detected
  - 2 phases
    - Internet Key Exchange Phase 1
      - heavy, slow, asymmetric
      - authentication with password or cert
      - create shared symmetric key
      - results in IKE Security Association (Phase 1 Tunnel)
        - actually, one in each direction
      - uses Diffe-Hellman Key Exchange *shrug*
        - essentially, each side can generate the same key independently, using their own private and other side's public keys
    - IKE Phase 2
      - establishes agreed upon encryption method and keys used for the actual data transfer
        - pick best method that they both support
      - DH key and other exchanged data (like random numbers) is used to create the actual stronger IPSEC key
      - results in Phase 2 Tunnel running on top of phase 1
  - Phase 2 can be torn down and still leave Phase 1 up for reestablishing phase 2 efficiently

IPSEC VPNs
  - 2 kinds, differ in how they detect Interesting Traffic
    - Policy Based
      - rule sets match traffic, use SA pair
      - can have different security settings for different traffic between same peers
        - these phase 2 settings can all be based on same phase 1 tunnel
    - Route Based
      - matches CIDR range, one SA pair per range (prefix)


======================
Virtual Private Cloud
======================

- Virtual network
- Created within one region and one account
- by default, private and isolated from public AWS zone and other VPCs
- VPC CIDR defines range of IPs in use
- Subdivisions of VPC (Subnets) are each located in 1 AZ


Default VPC is a pre-configured thing that we don't use
  - only one per region per account
  - CIDR is 172.31.0.0/16
  - One subnet per AZ in region, /20 size
    - plenty of spare room
  - Internet gateway, Security Group, NACL created
  - Anything placed in default subnet gets a public IP. Different from Custom VPC
  - can be deleted / recreated
    - Some things expect it to exist tho

Design considerations
  - are there networks we have to avoid
    - other vpcs, cloud, on-premises, partners/vendors
  - what size range
    - vpcs can be between /28 (16 ips) and /16 (65536 ips)
  - how many ranges
    - how many subnets
      - how many AZs?
        - three per region seems good, plus a buffer for a total of 4 subnets due to AZs
      - how many tiers? a common pattern is
        - public web tier
        - private with gateway application tier
        - private with no gateway database tier
        - let's add a spare
      - So, 16 subnets per account. if starting with /16 vpc, this would be /20 each subnet.
        - starting with /18 would make /22 subnets
      - how many accounts?
        - Prod, dev, general, spare
      - we need 48 ip ranges
  - the class lost me here re: the A4L example. it started talking about having 4 vpcs per region/account but also all AZs and all Tiers as subnets in 1 vpc
    - not sure where the 4vpcs come from. just being big?
  - don't forget we need a vpc per region
  - try to predict the future (?)
    - is there more to that than "leave spare room"

Custom VPC
  - isolated; nothing in or out without explicit permissions
  - hybrid networking: on-premises + cloud
  - 2 hardware tenancy setups
    - Default
      - shared hardware
      - can choose per-resource to have dedicated hardware
    - Dedicated
      - can no longer choose shared hardware
      - more expensive
  - everything gets private IP, public ips are given when we need them
  - one primary IP4 CIDR /16 block
    - up to 5 (more with support) secondary cidr blocks
      - these are if you run out of space, essentially
  - one optional IP6 /56 CIDR block
    - can't choose block, can supply ones you already own
  - DNS
    - provided by Route53
    - dns ip is base ip + 2
    - VPC-level options
      - EnableDnsHostnames
        - because ips can be dynamic and this will resolve correctly
          - why did I not know this while working on reply-guy?
      - EnableDnsSupport
        - enables or disables DNS resolution

Subnets
  - AZ Resilient, inside 1 AZ
  - ip4 cidr is subset of vpc cider
    - optional ip6 /64 range, 1/256th of the VPC ip6 range
  - subnets in a vpc can't overlap
  - by default, can freely communicate between subnets within a vpc
  - reserved addresses
    - first is vpc itself, network address
    - first + 1 is vpc router
    - first + 2 is dns address, helps use hostnames internally and externally
    - first + 3 is reserved for future use
    - last is Network Broadcast address which talks to every host
      - broadcasting is not actually useable in VPC
  - for every vpc there is a DHCP option set
    - DHCP assigns IP addresses
    - can't edit options, can create new option set instead
    - applies to all subnets in vpc
    - options
      - Do we give resources a public ipv4 address by default?
      - Do we give resources a ipv6 address?

VPC Router
  - every VPC has one
  - network + 1 ip
  - Highly available, runs in all the AZs the VPC uses
  - routes traffic between subnets
  - controlled by Route Tables
    - can be associated with many subnets
    - each subnet has one
      - default one is the VPC-level Main Route Table
    - list of routes, which have destinations which can be ip ranges (or /32 for 1 ip address)
      - if multiple routes match incoming packet, most specific applies
      - each route has a "target", either "local" or an AWS Gateway
        - local means within VPC
    - each route table has the "local route"
      - matches VPC CIDR range with target "local"
      - if enabled, also has ipv6 local route
      - LOCAL ROUTE ALWAYS TAKES PRIORITY
    - then what are non-local routes good for? some examples
      - traffic out to internet
        - instead of default internet gateway, could route to vpn
      - necessary for cross-vpc communication
      - necessary for private subnet to send traffic out to internet

Internet Gateway
  - Region Resillient
  - IGW can have 1 or 0 VPCs and vice versa
  - runs in AWS Public zone, coordinates traffic to/from internet or public zone
  - steps
    - create IGW
    - attach to VPC
      - now we can use it in route tables
    - create and attach custom route table to subnet
      - create default routes 0.0.0.0/0 and ::/0 to the IGW
  - whenever a resource has a public ip, the internet gateway actually maintains the map to the private IP
    - ONLY V4. V6 is natively publically routeable
    - changes packet destination or source ip to public ip
    - this is why instance OS doesn't know public ip address
  - Egress-only
    - special kind of IG, allows only inside-to-outside for IPv6
    - problem is solved with NAT for ipv4 but NAT doesn't work with v6
    - architecturally, subnet route table default route ::/0 points to EOIG

Bastion Host = Jumpbox
  - instance in subnet
  - manage routing, sometimes the only way into highly secure private VPCs

Flow Logs
  - capture packet metadata
  - can capture on accepted, rejected, or all packets
  - can be applied to whole VPC, one Subnet, or one ENI
  - not real-time
  - can go to s3, cloudwatch logs, or athena (sql queries, "schema on read" whatever that means)
  - each capture entry is a set of fields like "bytes", "srcaddr", "dstport", "protocol", and "action"
    - Action is accept, reject
    - protocol is 1:ICMP, 6:TCP, 17:UDP
  - exclude metadata service, time server requests, DHCP, Amazon DNS servers and Amazon Windows License

VPC Endpoints
  - Gateway Endpoint
    - special way for private networks to access S3 and DynamoDB
    - created per service per region
    - prefix list (aws-managed list of service ips) is added to associated subnets
      - point to the GE (instead of the internet gateway)
    - Endpoint policy controls what buckets GE can access, for instance
    - can't access cross-region services (bucket in other region)
    - enables private s3 buckets that never touch the internet
    - enables completely offline vpcs that can still use s3
    - only accessible from inside vpc
  - Interface Endpoint
    - provide private access to non-DynamoDB services
      - example: Instance Connect has an endpoint you can deploy to access private instances
    - take the form of ENIs inside Subnets - not HA
    - can use security groups in addition to endpoint policies
    - only ipv4 and TCP
    - uses PrivateLink, which allows things (even 3rd party things) to be injected into VPC
    - endpoint is given a regional and zonal DNS name app can use
      - or PrivateDNS, a Route53 hosted zone that overrides default service dns names with endpoint ones

VPC Peering
  - direct encrypted network link between exactly 2 VPCs
    - not transitive
  - can be different regions & accounts
    - cross-account peering connection is an invite/accept thing
  - can optionally make public service hostnames resolve to private addresses
    - doesn't work if there's CIDR block overlap
    - so, can use same hostname for an EC2 instance, say, inside and outside network
    - reasons not to: the other VPC now having your internal IPs, potential name conflicts
      - also it might encourage bad practices like accessing internals unnecessarily
  - same-region Security Groups can reference peer SGs instead of IP ranges
  - need routing configuration sending traffic to peering gateway object (on both sides)
    - and obviously NACLs and SGs need to allow traffic


==============
Cloudformation
==============

Does cloud stuff based on config file
  - In YAML (maybe json?), Description field, if used, must directly follow AWSTemplateFormatVersion
  - Metadata field controls UI among other things
  - Parameters add options for user to select when using CFN UI
  - Mappings makes lookup tables
  - Conditions are a thing
  - Outputs are like, what is the resulting ec2 id and stuff, probably goes in a log somewhere

CFN templates create stacks (a template can start many stacks). Stacks make stuff (ec2 instance, etc.)
  - Deleting stack deletes the stuff
  - Template -> logical resource (stack) -> physical resource (stuff)


==============
Route53
==============

Domain Registrar and Hosted Zones (managed nameservers)
Globally resillient, no region

Hosted Zones (managed nameservers) and general DNS flow
  - Can be
    - Public
      - accessible to public internet (duh)
      - Hosted on 4 nameservers which have the resource records on them
      - godaddy or route53, say, tells the .com TLD server, say, about your nameservers
        - if external registrar, you need to tell godaddy to tell .com to point to your r53 nameservers
          - uses NS record
      - vpc by default uses a Route53 resolver for dns queries
        - things in VPC can use this, VPC + 2 address
        - still uses public root dns servers for public hosted zones
      - monthly cost and tiny charge for queries
    - Private
      - Linked to VPC in your account, can link cross-account but not through UI
        - Route53 resolver first looks for private hosted zone before sending request to root dns server
      - can do split-view hosting where the same name has both a private and public host
        - could have a subset of records in public zone and all records in private zone
          - maybe some subdomains don't resolve publicly
        - could make a company website that appears different over vpc
          - seems horrible but what do I know

Special type of resource record: ALIAS
  - can point apex (amazon.com) to AWS service
  - resolves to an A or AAAA record, you specify
  - useful because, for instance, Elastic Load Balancer doesn't give you an IP but a name that points to one (an A record)
    - can even point to an S3 bucket, other things
  - no charge to use

Health Checks
  - separate from records, but records can use them
    - routing other than simple routing doesn't return unhealthy records
  - fleet of health checkers
    - don't block access by them if you want health check
  - check every 30s or 10s for a fee
  - support TCP, HTTP(S), HTTP(S) with string matching
  - can be healthy or unhealthy
  - healthy check must establish connection in 4 seconds and receive body in 2 more
  - healthy string match check must contain string in first 5120 bytes
  - Endpoint checks, Cloudwatch Alarm checks, or calculated checks (from other checks)


Routing types:
  - Simple Routing
    - one record per name
      - can have multiple values
      - consumer gets all of them randomly and chooses one, can try others on fail
    - good for sending all traffic to one logical place, like one of a few identical servers, or whatever
    - no health checks
  - Failover Routing
    - health check on primary record
    - returns secondary record if primary health check fails
      - maybe a maintenance page
  - Multi-value Routing
    - multiple records for same name
    - up to 8 are returned randomly, not those with unhealthy checks
    - improves availability
    - not a replacement for load balancing
      - client still picks ip to use
      - load balancer has more features
        - more advanced health checks, sticky sessions, etc.
  - Weighted Routing
    - records with same name returned proportional to weight
    - quirk: if all records are zero, equal weight to all
    - good for testing new versions (just give 5% of users this version)
    - unhealthy chosen record restarts process
  - Latency-based Routing
    - one record of a particular name in each region
    - lowest latency region is automatically chosen
      - not lowest latency resource, just general region latency
      - you specify what region the resource is in
        - could make a mistake here! no validation of that.
    - if unhealthy chosen record, use next lowest latency record
  - Geolocation Routing
    - records are tagged with location: country, continent, default, or subdivision (state)
    - only most specific relevant record is returned, or default record, or "NO ANSWER"
  - Geoproximity Routing
    - gives record with lowest physical distance (with configurable bias) to requester
    - define rules with
      - region if AWS resource, lat + lon if external
      - bias that expands the circle area that a location takes up for distance calculation
        - can move midpoint only on external resources (by shifting lat/lon)

Interoperability
  - Registering Domain OR hosting zone files, not both
  - Can register domain in one place, then set up nameservers in another
    - give registrar those servers' info, registrar tells TLD registry about it
    - R53 can be either party here


=========================
EBS - Elastic Block Store
=========================

EBS
  - Block network Storage attached to ec2 instance 
    - raw data, instance creates file system on top
    - lifecycle is independent from ec2 lifecycle, gets attached and detached
  - Can be encrypted via KMS
  - Provisioned per AZ
    - can get toward region resilience by backing snapshot up to S3
    - then why not replicate that to another region?
  - Generally one ec2 instance per EBS Volume
    - multi-attach can be managed but has to work to avoid corruption, overwrites, etc.
  - Billed GB/mo

Volume types:
  - GP (General Purpose)
    - types
      - GP2
        - 1GB to 16TB
        - Billing has credit bucket architecture
            - bucket has capacity of 15MM IO credits
              - gets that at start
                - nice for boots / initializations
              - no credit no IO
              - IO credit is 16KB
            - refills at Max(100,  3 * GB of storage) credits/sec
              - "baseline performance"
            - initial credits
            - can do up to 3000 IOps for <1TB volumes
              - after 1TB, max speed scales as fast as refill so doesn't deplete
              - caps at 16000 IOps for 5.33TB volume

      - GP3: base 3000 iops 125MiB/s, can pay for more
        - simpler, max 4x faster, cheaper
        - extra speed added manually
        - Best of both worlds between IO1 and GPT2
    - good for Virtual Desktops, medium sized DBs, low latency apps, boot volumes

  - IO (Provisioned IOPS)
    - types
      - IO1
        - up to 4x the top throughput as GP (same IOps)
        - speed independent of size
        - for low latency + consistency
      - IO1
        - speed independent of size
        - for low latency + consistency
      - Block Express
        - up to 4x the top throughput and IOps as IO1 & 2
    - Per instance limits apply when multiple volumes are attached to an instance
      - io1 and io2 block express are 260000IOps and 7500MiB/s
      - io2 is lower
      - instance type might have its own bottleneck

  - HDD
    - slow, cheap
    - can't boot from
    - 2 relevant types
      - st1: Throughput Optimized
        - good for sequential operations, not random access
          - big data, log processing
        - bucket system like gp2, base + burse
          - max 500MB/s
            - 40MB/s/tb base (refill rate)
            - 250MB/s/tb burst
      - sc1: cold
        - cheapest, for archives
        - max 250MB/s
          - 12MB/s/tb base
          - 80MB/s/tb burst

  - Encryption
    - OS doesn't see encryption
    - can't unencrypt a volume
    - create encrypted volume initially
      - KMS key is used
        - could be default EBS-managed key "aws/ebs"
        - could be customer-managed KMS key
          - KMS key creates encrypted DEK and KMS key is used to decrypt that
            - DEKs are per volume
            - decrypted DEK is stored in EC2 instance memory
            - now instance can decrypt volume
            - snapshots made from this volume (and volumes made from them) also use this DEK








=========================
EBS Snapshots
=========================

- Moves from EBS' availability zone resilience to S3 (region resilient)
- Incremental (think diffs)
- can be copied to other regions, can restore volumes
- restores are lazy, performance can suffer soon after restore
- FSR Fast Snapshot Restore costs extra, avoids laziness and you can have 50 per region
  - can just force the reads manually instead
- Billed GB/month
- snapshots only billed USED data on volume


=========================
IAM
=========================

Identities are IAM users, IAM groups (sort of), and IAM roles 

Identity policies are attached to Identities

ARN
  - unique identifers (unique within account)
    - most things unique across account + region
    - some things like IAM users unique within account
    - some things like s3 unique globally
  - slightly varied format
    - arn:aws:service:region:account-id:resource-id
    - arn:aws:service:region:account-id:resource-type/resource-id
    - arn:aws:service:region:account-id:resource-type:resource-id
  - "aws" is always the same for commercial aws (not government, etc)

Policy document (config) has statements with
  - action
    - "servicename:action", also can be specific action or list of them, including wildcards
  - resource
    - same as above, but instead of servicename:action, arn is used
  - effect 
    - allow or deny, conflicts are resolved in priority order
      - deny
      - allow
      - default is deny
  - optional Sid

Identity can have multiple policies
  - Someone can have 2 or more policies attached to them, and be in a group with another policy attached
    - same rules apply. If the group is explicitly denied, the user can't access it even if their personal roles specifically allow it.

Inline policies are not best practice
  - have to change it everywhere it's applied
  - can be good for exceptions when one identity needs special treatment
Managed Policy instead
  - create policy object and assign it to many things

IAM users are an identity used for long term AWS access (people, apps, service accounts)
  - if you can point to a named thing that needs access, it probably wants to have a user
  - Principal is the thing trying to authenticate as a user

5000 Users per account
  - for big companies, roles & identity federation can work
User can be in 10 groups

IAM Groups are containers for users
  - you cannot log into a group
  - like users, groups can have inline or managed policy attached
    - managed policies can be AWS-managed (built-in) or customer managed (custom)
  - no limit to how many users in group
  - no default "all users" group
  - no groups nested in groups
  - 300 groups limit
  - resource policy can't grant access to a group
    - they aren't true identities

IAM roles
  - temporary, a hat you put on for a short job
    - temp credentials are given via STS
  - roles have Trust policy and Permissions policy
    - Trust policy specifies who can assume role
      - can be users, services, can be used anonymously
    - Permissions policy specifies what permissions the role has
  - use cases
    - give aws services like lambda permission to stop ec2 instances or whatever
    - emergency role for break glass scenarios
    - existing identity provider (web identity federation)
      - no aws credentials stored in application
    - > 5000 staff
    - cross-account, give other account permissions
  - service-linked role
    - predefined role linked to an aws service
    - can't delete unless service is no longer using it
    - PassRole action permissions let you pass service-linked roles to other services
      - When an AWS service needs to assume a role, a user must have iam:PassRole permissions to pass that role to the service.


=========================
Organizations
=========================

Groups accounts
  - subgroups under Organizational Root are Organizational Units
    - can be nested

The account that makes the Organization is the management/master/payer account
  - can't be restricted
  - can invite accounts to join org
    - those accounts go from being standard accounts to member accounts
      - consolidated billing: Billing passes through to payer account
        - consolidates volume discounts

can create accounts directly in organizations
best practice is that only one account is logged into (maybe master acct, maybe not)
  - other accounts have roles this account can log into (role switching)
SCPs manage permissions for member accounts
  - can be attached to accounts, OUs, or Org root
  - inherit down
  - can't affect management account
  - can indirectly restrict account root user (not usually possible)
  - don't grant permissions, only defines what COULD be allowed
    - default SCP is FullAWSAccess
      - doesn't actually grant permissions, identity policies still needed


=========================
Cloudwatch
=========================

Public zone service
Regional
log = info + timestamp
many aws services are integrated by default
log stream is a sequence of log events from the same source
log group is a container of log streams
  - settings are set here, like retention settings
  - can generate metrics from log data
    - looks for patterns, increments metric, can trigger alarms

Has UI, CLI, and API interfaces (i imagine most services do)

Metrics
  - a metric is a time-ordered set of data points
    - Dimensions are used to identify
      - EC2 InstanceId could be a dimension, but so could InstanceType. Depends what they think someone might want to look at
      - can't add dimensions to native metrics, only custom
  - some things are automatically gathered (ec2 cpu usage)
  - some external things need Cloudwatch agent to add metrics

Logs
  - same same but different

Events
  - can perform actions based on metrics
  - can perform actions at a certain time
  - can for instance send an email based on an alarm

Namespace
  - custom for external data
  - "AWS/EC2" (for instance) for native things

logs API calls and aws activities as cloudtrail events
  - can be action taken by user, role, or service
  - stored 90 days by default, free
  - can be 1 of 3 kinds of events
    - management events
      - creating/terminating instances, create vpc
      - enabled by default
    - data events
      - objects added to s3, lambda invoked
      - not enabled by default because it's a lot!
    - insight events

can create trails for customization
  - logs events in a region
  - can be configured to log global service events
    - enabled by default
    - consolidated into us-east-1 specifically
  - can be all-region, which is basically just one in each region operating as one logical trail
    - so changes to configuration will apply to all regions and new regions are added automatically
  - can be stored in S3 as json
  - can be stored in Cloudwatch Logs as well
    - search through, use metric filters
  - you can make an organization trail using the org management account
    - for consolidation

There is a delay!
  - within 15min usually


=========================
AWS Control Tower
=========================

Orchestrates organizations and other services to set up a multi-account environment

Landing Zone
  - has home region
  - uses organizations, AWS Config, Cloudformation, etc. to work
  - like organization, has management account
  - sets up SSO, centralized logging
  - creates 2 OUs
    - Foundational "security"
      - creates 2 accounts
        - Audit
        - Log archive
          - users who need all logging info
    - Custom "sandbox"

Guard Rails
  - 3 types: Mandatory, Strongly Recommended, Elective
  - Function in 2 ways
    - Preventative: uses SCP to stop things from happening
    - Detective: uses AWS Config to detect when things happen
      - clear, in violation, or not enabled

Account Factory automates and standardizes new account creation
  - guard rails can be auto applied
  - account admin given to named user
  - configure network like ip ranges
  - accounds can be deleted
  - can be integrated into a business' SDLC


=========================
S3
=========================

Security
  - Private by default
  - Bucket policies
    - type of resource policy
    - how you give access to something to identities outside your account, since you can't attach identity policies to them
    - can grant access to anonymous principals
    - statements look like Identity policy's, with additional Principal field
    - can block by conditions like matching IP, uses MFA, more
  - Access Control List
    - Legacy, not recommended
    - Subresource (like object is to bucket)
      - can be attached to object or bucket
    - inflexible
      - no conditions
  - Block Public Access
    - apply only to anonymous principal
    - can block
      - everything
      - things granted by new ACLs
      - things granted by all ACLs
      - things granted by new resource/access point policies
      - things granted by all resource/access point policies

Static Website Hosting
  - need to point to an index and error page
  - if you want to use custom domain, bucket name needs to match
  - good for offloading image serving, etc. from ec2

Object versioning
  - controlled at bucket level
  - can be enabled and suspended, but cannot be disabled
    - suspending and deleting existing versions manually is kinda like disabling
  - does what you expect, stores multiple versions
  - version number is called "id", compared to the object "key" (like a filename)
  - can request specific version by ID
  - when we "delete" an object and don't give an ID, the current version becomes a delete marker (hides all versions)
    - can delete the delete marker
    - specifying id does actually delete the version
  - MFA Delete
    - must use MFA to delete versions and to suspend versioning (or enable it)

Perforance Optimization
  - Single PUT upload can be unreliable
    - needs to restart upload if connection drops
    - limited to 5GB, which is already ridiculous
  - Multipart Upload
    - minimum size is 100MB
    - 10000 max parts between 5mb and 5gb (except last remainder part)
    - each part can be restarted
    - faster
  - Transfer Acceleration
    - helps problem of public internet and ISPs tossing our upload all over the place before it gets to S3 instance we want
    - gets to closest AWS edge location via public internet, then tunneled via AWS-controlled network straight where it goes
    - off by default
    - bucket name must be dns compatible, with no periods
    - more benefits as distance increases between uploader and destination bucket

Encryption
  - Buckets are not encrypted. Objects are.
    - each object can use different encryption settings
  - encryption-in-transit is used, can't see inside the tunnel
    - some exceptions
  - client-side encryption-at-rest
    - just upload some encrypted data lol
    - no longer supported by itself, everything is encrypted
      - of course, they can't prevent you from doing both
  - server-side encryption-at-rest
    - unencrypted objects inside encrypted tunnel are encrypted by S3 when they get there
    - 3 types
      - SSE with S3-Manged keys (SSE-S3)
        - default
        - you provide plaintext object and S3 makes a key
        - no options, invisible
        - per-object key is further encrypted by master S3 key, plaintext discarded and cyphertext per-object key saved
        - can't stop S3 full admin from seeing all data
          - might be illegal re: medical records, etc.
          - no Role Separation
        - uses AES-256 encryption
      - SSE with keys in KMS (SSE-KMS)
        - uses KMS key that you can configure
        - this takes the place of the master S3 key above
        - have logging and auditing against key using Cloudtrail, etc.
      - SSE with Customer-provided keys (SSE-C)
        - you provide key and object
        - s3 encrypts object and hashes key + object, attaches hash to object
        - s3 discards key
        - managing key creation is good for high regulatory environments
  - Bucket Keys
    - without bucket keys, encryption process for SSE-KMS requires a lot of calls to KMS, multiple per upload
      - There is throttling with KMS keys, too, so it isn't just an efficiency thing
    - KMS key is used to create a time-limited bucket key, which can be used to generate DEKs per 
      - this works because the bucket key is re-deriveable from the KMS key.
      - Only the KMS key is needed to decrypt
      - Kinda seems the same as caching the KMS key in memory, but transparently with best practices
      - Fewer KMS Cloudtrail logs, and they will have the bucket ARN, not object ARN
  - Replicating plain text objects to a bucket with bucket key or default encryption can change the "ETAG"

Storage Classes
  - S3 Standard
    - Duplicated across at least 3 AZs
    - "11 9's" of durability, 1 lost object every 10,000 years
    - replication is checked for accuracy with CRCs
    - if an object is durably stored like this, HTTP/1.1 200 OK status.
    - pay for storage gb/m; transfer OUT per gb, not in; and price per 1000 requests
    - can be made public, no minimum duration or size
    - first byte latency in ms, fast 
    - balanced and good for frequently accessed data that is important and non-replaceable
  - S3 Standard-IA (Infrequent Access)
    - cheaper to store, but has retrieval fee
      - retrieval fee (basically an access fee) but not transfer fee could apply to data that never leaves AWS (accessing a log for instance)
    - minimum duration billed: 30 days
    - minimum billed object size: 128KB
    - good for few accesses of important, irreplaceable data
      - once per month?
  - S3 OneZone-IA (Infrequent Access)
    - Like above without replication
    - cheaper still
    - for data that can be replaced
    - Basically never fails but you wouldn't bet your business on it
  - S3 Glacier-Instant
    - Like Standard-IA, but cheaper to store, more expensive to access, and 90 day minimum
    - good for data accessed once per quarter
  - S3 Glacier-Flexible
    - Formerly S3 Glacier
    - 1/6 the cost of S3 standard
    - not immediately available
    - can't be made public, including static website hosting
    - retrieval process needs to be done in advance of accessing
      - temporarily puts them in S3 Standard-IA until accessed
      - 3 types of retrieval
        - Expedited: 1-5 min
        - Standard: 3-5 hours
        - Bulk: 5-12 hours
        - Faster = more expensive (duh)
    - 40kb min size
    - 90 day min duration
  - S3 Glacier-Deep Archive
    - cheaper yet
    - like above but 180 day min
    - Standard retrieval: 12hr
    - Bulk retrieval: 48hr
    - good for regulatory archives, stuff that shouldn't ever have to be retrieved but needs to be kept
    - good as secondary backup
  - S3 Intelligent Tiering
    - 5 levels kinda map to above
      - Frequent
      - Infrequent
      - Archive Instant
      - Archive 
      - Deep Archive 
    - monitors usage and moves from tier to tier intelligently
    - Archive and Deep Archive require application changes and has same "dethawing" process
      - They are optional
    - Tiers higher than Archive do not incur retrieval fees
    - monitoring and automation fee instead
    - good for long lived data with unpredictable usage spikes

Lifecycle Configuration
  - applied to bucket or group of objects (prefix, tags)
  - rules consist of actions
    - can't trigger based on access patterns
    - 2 types
      - Transition actions change storage class (say, move to IA after 30 days)
        - can transition from more expensive -> less  
          - Intelligent Tiering is between IA and OneZone IA
          - weird exception: can't transition from OneZone IA to Glacier Instant Retrieval
        - remember that transitioning small objects could incur cost bc min sizes
        - Transitioning from Standard requires object to be in Standard for 30 days
          - can still transition manually tho
          - weirdness: doing so to Standard-IA or OneZone-IA starts another 30 day timer until you can move down to glaciers
            - BUT ONLY IF YOU WANT TO DO IT WITH ONE RULE??
      - Expiration actions delete after a while

Replication
  - cross-region or same region replication
  - can replicate to bucket in other account
      - destination bucket needs to have bucket policy to trust source account
  - Replication configuration
    - destination bucket
    - IAM Role to use
    - all objects or a group of them (prefix, tags)
    - optionally change storage class
    - ownership (what account owns object)
      - might want to make this the destination account if cross-account replication
    - RTC (replication time control)
      - Speeds up to < 15min vs "best effort"
      - costs extra
  - not retroactive by default
    - can configure "batch replication" to replicate existing objects
  - must enable versioning on source and destination
  - one directional by default
  - can handle encrypted objects, though extra config needed for SSE-KMS.
  - source bucket needs permissions for objects
  - won't replicate changes to lifecycle config, tags by default, other system events
  - can't replicate Glacier Flexible or Glacier deep archive
  - no delete markers by default
  - use cases for same Region 
    - resilience where regulations means data stays in one region
    - could use to aggregate multiple buckets' logs
    - sync some prod and test data
  - use cases for cross Region 
    - global resilience
    - reduce latency

Presigned URL
  - to create, someone with permissions supplies credentials, bucket + object, expiration time
  - allows unauthenticated request for object upload/download
  - when given the URL, unauthenticated user operates on object as the person who generated it
    - permissions are NOT baked in at generation
  - this is how you would have a website with videos only for logged in users
    - web server asks s3 for presigned url to send to logged in user
    - otherwise, videos would have to be public
  - you can generate a useless presigned url by not having access to the specified object
    - permissions can change shortly after to make it useful
  - don't generate using an IAM role (temp role credentials could expire at unpredictable times and break the URL)

S3 Select and Glacier Select
  - use sql-like statements to retrieve parts of objects
    - data stored as JSON, CSV, others
  - because client side filtering isn't cost-effective for big objects

S3 Events
  - older feature, eventbridge might be better
  - Can be delivered to SNS topics, SQS queues, lambdas
  - can trigger on put, post, copy, CompleteMultiPartUpload, deletes, restores, some replication stuff
  - Event Notification Config
  - lambda, etc. needs resource policy to allow S3 principal to interact
    - weirdly no way to edit lambda resource policy via UI?

S3 Access logs
  - log accesses of source bucket to some other target bucket
  - best effort, few hours
  - happens through "log delivery group"
    - built-in service principal
  - target bucket needs ACL to accept from LDG
    - bucket policies can't specify built-in AWS service principals, go figure
  - newline-delimited records with space-delimited attributes
  - logging configuration sets prefix for logfile
  - need to manually delete

S3 Object Lock
  - Existing buckets need support to turn on
  - versioning required; individual versions are what is locked
  - means no overwrites or deletes of that version
  - 2 ways it manages retention, both or either (or none) can be active
    - Retention Period
      - specify days and years to retain.
      - 2 modes
        - Compliance Mode
          - retention period / lock itself cannot be reduced, even by account root user!
        - Governance Mode
          - similar, but you can get special permissions to remove the lock
    - Legal Hold
      - Not time-based, just on or off
      - For when a version is critical
  - Bucket default settings are possible in addition to individual vesion settings

- S3 Access Points
  - Intermediary between principal and bucket
    - simplifies access management
      - instead of monolithic bucket policies, different groups with different permissions can access same bucket
  - can be set to be accessed only directly from within vpc
    - needs vpc endpointin vpc
  - for some reason CLI command is important to remember: "aws s3control create-access-point"
  - access points have unique DNS addresses
  - common pattern is for bucket policy to allow everything from a given access point, then use access point policies to actually manage permissions


=========================
Key Management System
=========================

Regional and Public
  - multiregion keys are supported, off by default
Create, store, and manage keys for cryptography
  - both symmetric and asymmetric
Perform encrypt, decrypt, etc.
Keys never leave the service
Compliant with FIPS 140-2 level 2
KMS Keys
  - formerly CMKs
  - contain id, creation date, key (resource) policy, description, and state
  - container for actual key data
    - can be generated or imported
    - up to 4kb of key data
  - granular permissions: decrypt, create, encrypt, etc. are separate
  - Support rotation, on by default
    - rotation mostly invisible
    - old versions are kept so you can decrypt pre-rotation encrypted files
  - can alias, so that you can swap keys
Data Encryption Keys
  - DEKs use KMS Keys
    - linked to KMS key
  - work on > 4KB
  - KMS doesn't operate on DEKs, it gives it to you to work with
    - plaintext
    - cyphertext
      - can be used with linked KMS key to get plaintext DEK
    - workflow:
      - encrypt data using DEK plaintext
      - discard plaintext DEK
      - store encrypted DEK with data
Key Policy
  - every key has one
  - unlike other services, keys don't automatically trust account they're in
    - usually, you explicitly grant access to the account then use IAM to grant usage rights
    - in higher security situations, you might skip IAM and grant permissions directly in key policy
  - there's another thing called grants


=============================
Network Access Control List
=============================

Firewalls for VPC (see General -> Firewalls)

NACL
  - Stateless
  - Every subnet has one
    - connections within one subnet are not filtered
  - supports explicit allows and denies
    - Can block bad IPs for instance
  - supports ip and port ranges
  - NOT specificity based, but rather the first matching rule applies
    - * is the fallback, basically the same as using the last allowed rule number
      - "implicit deny", can't be changed 
  - managed at vpc level but can be attached (only) to subnets
  - default NACL allows all, customs are created with deny all 
  - can't reference logical resources; ips and ranges are all you get!


=============================
Security Groups
=============================

Security Group
  - Stateful
  - implicit deny, explicit allow
  - No explicit deny: needs NACL to override the useful explicit allow
  - attached to Elastic Network Interfaces
  - doesn't attach to instance, attaches to its network interface
  - supports logical resources including itself and other security groups
    - can set "source" to another security group's id
      - matches any source with the other sg attached (kinda weird)
      - can only reference SGs in same vpc or peered vpcs
      - scales well
    - can set "source" to same security group's id
      - anything with sg attached can talk to each other or whatever


=============================
NAT Gateway
=============================

Network Address Translation
  - For giving private resource outgoing-only access to the internet
    - called "outgoing-only" but reponses are still allowed.
  - changes packets' src or dest ip
    - like how internet gateway does (static NAT) with public ip addresses being changed to internal instance IP
    - IP Masquerading: hiding CIDR blocks behind one IP
      - lets ipv4 addresses conflict without problem as long as the "mask" is unique
      - breaks incoming access
  Architecture
    - App tier vpc has private instances with private IPs, wants to update software
    - Web tier vpc has NAT Gateway
    - App tier route table points traffic to nat gateway instead of internet gateway
    - NAT Gateway records private source IP, other data in translation table
      - table entries (requests) are differentiated by choosing a port to receive response on
    - Changes source IP to its own, sends data to Internet Gateway and out
    - gets reponse, uses translation table to adjust response destination back to private IP
  - AZ Resilient (deploy into a region)
    - for region resilience, you need
      - NG in each AZ
      - route table pointing at it in each AZ
  - Managed service, scales to 45Gbps
  - billed for duration you have NAT Gateway and Data Volume
  - doesn't work with ipv6
    - use "Egress Only Gateway" instead

NAT Instance
  - instead, can do NAT with EC2 instance
  - more customizeable
  - can do port-forwarding
  - can use security groups instead of just NACLs


==============
EC2
==============

Architecture
  - Virtualization As a Service
  - AZ resilient
  - shared or dedicated hosts
    - "host" is actual hardware in AWS Datacenter
    - even shared hosts are isolated, but dedicated might be required by regulations
  - When instance is deployed to a subnet
    - it gets assigned a primary Elastic Network Interface in that subnet.
      - instances can have multiple network interfaces, even in other subnets, but must be same AZ
    - might connect to EBS
  - Instance (on shared host) changes host on stop/start or failure (not restart)
  - Network cards, EBS volumes, instance resources are all in AZ
    - can't connect ENI from one zone to instance in another for instance
  - usually instances of same type and generation will share a host

When to use
  - You have a traditional OS + App compute need
  - long-running
  - burst or steady state
    - many kinds of instances, one will suit your needs
  - monolithic stack
    - need certain database running, whatever
  - easy migration point to the cloud
  - good for disaster recovery
    - can recovery using AMIs, architect something with redundancy easily
  - sorta the default

Instance Types
  - Choosing an instance type affects certain things
    - resources (cpu, ram, etc)
    - bandwidth (storage and network)
    - system architecture (arm, x64), vendor (amd, intel)
    - additional features like a GPU
  - Categories
    - General Purpose
      - default, diverse workloads, balanced resource ratio (ram to cpu, say)
    - Compute Optimized
      - High CPU use: ML, Modeling, gaming, processing
    - Memory Optimized
      - High RAM use: large in-memory datasets, certain database workflows
    - Accelerated Computing
      - Niche uses: hardware GPU, field programmable gate arrays, etc.
    - Storage Optimized
      - Fast sequential and random IO
  - Example instance type name: R5dn.8xlarge
    - R: instance family
    - 5: 5th generation
    - dn: letters mean additional capabilities.
      - d is SSD storage (for some reason)
      - n is network storage
    - 8xlarge: size. like a 8X tee shirt, big!
    
Instance Store Volume
  - Block Storage connected to instance
  - local, physical
  - comes with price of instance
  - have to be attached at launch, not later
  - many types, some higher end types are D3 at 4.6GB/s, and I3 at 16 
    - IOps are crazy bigger

Networking
  - Every instance has 1 primary ENI in its subnet, can attach more secondaries (can be in other subnets)
    - secondaries can be attached/detached
      - if license is attached to mac address, this can be useful
    - multiple ENIs can mean multiple security groups
  - ENI owns things that you might think instance owns
    - MAC address
    - primary private IP (comes with dns name that resolves to it)
    - optional public IP (comes with dns name that resolves to private ip inside VPC, public ip outside)
    - more secondary IPs
      - can have one Elastic IP per private IP
        - will remove regular public IP
    - IPs change on host change.
  - security group affects all IPs on interface
  - can disable source/destination check
    - setting discards traffic not to or from one of ENI's IPs
    - needs to be off to function as NAT instance
  - OS DOESN'T KNOW ABOUT PUBLIC IPv4 ADDRESS

Amazon Machine Images
  - regional with unique ami ids in each region even for preset ones
    - can be copied between regions
  - can't be edited, just create another one
  - used to launch images
    - even used to launch preset images
    - can be AWS- or community- provided (ubuntu is community for instance)
    - can be commercial (like coming with a windows license)
    - when you make one, you can make it public, private, or shared with some accounts
      - these permissions are stored in AMI
  - consist of image + attached volumes
    - does not contain volue data, only references them
    - snapshots are created
      - used in AMI using block device mapping
        - maps /dev/xvda (block device), say, to EBS snapshot
  - at launch, snapshots are made into volumes and volumes are attached to correct block device
  - optimally, for consistency, you stop the instance before creating an AMI

Purchase Options (launch types)
  - On-demand
    - default, how you think it works
    - per-second billing while instance is running
      - associated resources may still be charged when instance is stopped, such as storage
    - could be unavailable if there is a major EC2 failure
  - Spot
    - spot price goes up and down, you set maximum price you'll pay
      - if spot price goes up past your max, your instance is terminated
    - good for interruptable operations
    - can come at a huge discount
  - Standard Reserved
    - commit to certain amount of usage for 1-3 years
    - can reserve and not use, wasting money
    - you can launch instances where you reserve space
      - either per AZ or per region
        - AZ reserves capacity, region does not
    - partial discount on instance that doesn't fit in your reservation
    - different ways to pay
      - no upfront
      - some upfront
      - all upfront
      - what you don't pay is billed as per second cost
  - Scheduled Reserved
    - you can schedule regular but not constant usage
    - doesn't support all instance types or regions
    - must schedule 1200 hours/year for 1 year at least
  - On Demand Capacity Reservation
    - in case of low availablility (like due to failure) AWS prioritizes reserved purchases, then on-demand
    - ODCR puts you in the reserved group for this purpose, without the commitment
    - book for a specific AZ
    - starts immediately, no scheduling for future
    - can't reserve what's not available
    - good for
      - important but short term projects
      - temporary scale-ups
  - Dedicated hosts
    - you pay for the physical host, can launch into at will
    - have to manage host capacity
    - why?
      - might have licensing tied to cpu
      - stopping and starting instance stays on same host
  - Dedicated instances
    - middle ground
    - hardware isn't shared, but you don't manage host
    - hourly fee for regions where this is enabled, plus extra fee for instances
    - why?
      - regulations
      - don't have the extra management overhead of dedicated hosts
  - Savings Plan
    - hourly commitment for 1 or 3 years
    - reserve general compute dollar amounts or specific EC2 plan
      - general is better if you use notjust ec2
    - savings plan also works for lambda and fargate
    - products have special savings plan rate
    - if commitment runs out, you just pay normal rate

Status Checks
  - 2 checks
    - system status: host working correctly
      - power/internet outage, hardware issues, etc
    - instance status: instance working correctly
      - corrupt file system, OS issues, networking misconfiguration (like setting public ipv4 address in OS) etc.
  - can set cloudwatch alarm
    - can perform actions like reboot
      - or "recover," which moves to another host and rebooting. Unlike usual host migration, private ip address is retained
        - only works on system status errors
        - only works on some instance types
        - doesn't work if there's an instance store

Termination Protection
  - turned on per-instance
  - adds a second permission needed to terminate an instance
  - functions as an approval process potentially

Shutdown behavior
  - lets you terminate an instance on shutdown, if desired

Instance Metadata
  - accessible at specific ip 169.254.169.254/latest/meta-data/(attribute, such as public-ipv4)
  - can give lots of info including
    - ipv4 public address
    - temp credentials from instance role
    - no auth, not encrypted

Bootstrapping
  - user-data (startup script) is entrypoint, part of metadata
    - Bootstrapping allows for build automation, do things when launched
    - accessible at specific ip 169.254.169.254/latest/user-data
    - OS is configured to get user data from ip and run it
    - usually only run on launch (but in certain cases can be set to run on every boot)
    - run as the root user
    - OS needs to understand script
    - if script fails, instance will probably be status OK still, but configured wrong
    - not secure, don't put passwords in it ideally
    - max 16kb
    - use this for stuff that needs to be current, AMI baking for time-insensitive things
    - base64 encoded-- UI will do this but not CF template

    - cfn-init
      - script installed on instances that does config management
      - command to call it is passed in through user-data, like other script calls
      - declarative bootstrapping, describe desired state
      - handles
        - installing correct version of packages
        - manipulate OS groups and users
        - download zip files and extract them
        - create files with certain contents
        - run commands
        - control services
      - script gets config from CFN Resource metadata
        - in CFN template, EC2Instance[Metadata][AWS::CloudFormation::Init]
          - under that, things like "configSets", "configure_instance"
      - can watch for metadata changes and respond

    - Creation Policy
      - allows resource to report to CF that its configuration went well
      - in CFN template, example is EC2Instance[CreationPolicy][ResourceSignal][Timeout] = "PT15M"
        - in this example, CFN will wait for 15min to get a signal from cfn-signal
        - cfn-signal sends result of calling cfn-init
          - if it's an error, or the timeout elapses, stack is given an error status

Instance Roles
  - instance assumes IAM role with permissions policy attached to it
  - Instance profile allows apps in EC2 instance with assumed role to use permissions
    - in UI, created at same time Instance role is created (named the same)
    - in CF template or CLI, need to create separately
    - UI looks like you're attaching instance role to instance, but really you attach Instance profile
    - Instance profiles abstract away the explicit rule assuming that the instance would have to do
  - Temp credentials with Permissions are exposed via instance metadata
    - aws cli automatically looks for them there and uses them
    - rotated and valid, don't cache
    - iam/security-credentials/{role-name}
  - much better practice than putting keys in instance or elsewhere unsecured

System and Application Logging
  - Might want OS-level and application-level metrics
  - Cloudwatch and Cloudwatch Logs can't see inside EC2 instance
  - Architecture
    - Cloudwatch Agent runs in app and sends data to those services
      - Agent configuration tells agent what to capture
      - Just by installing and running agent with default config,you get extra metrics
        - you do have to generate default config with the wizard
    - Role attached to instance with Cloudwatch permissions
    - Log group for every log file we want to inject into CWL, log stream in each group does the logging

Placement Groups
  - influences instance placements
  - 3 strategies available
    - Cluster
      - keep instances close
      - For best performance
        - all members have direct connections to the others
        - can double bandwidth from 5Gbps to 10
        - should use enhanced networking and high-bandwidth instances to get max speeds
        - should use same instance type for best result
      - best to launch all instances at same time
      - first launch locks group to an AZ
    - Spread
      - keep instances far
      - good for resilience, why use redundancy if they aren't spread apart
      - instances guaranteed to be on a different rack from the others
        - each rack has its own network and power source
      - 7 instances per AZ limit
    - Partition
      - keep many (> 7) instances far
      - group instances into "partitions" that you want to keep far apart
        - max 7 partitions, each has its own rack
        - can send partition data to topology aware apps, which can make intelligent data replication decisions

Dedicated Host
  - nobody else uses it
  - can host a specific family and size of instances
    - newer nitro-based hostst can mix and match sizes
  - no additional instance charge
  - on-demand and reserve options available
  - host hardware has physical sockets and cores
    - can help with licensing
  - does not support
    - RHEL, SUSE Linux, or Windows AMIs
    - RDS instances
    - Placement groups
  - can share cross-account using Resource Access Manager

Enhanced Networking
  - virtualization-aware NIC splits itself into mini- network cards
    - uses SRV-IOV
    - host doesn't need to manage calls to a single logical card, can assign one per instance

EBS-Optimized Instance
  - storage IO doesn't affect data IO, stack has these separated somewhat
  - most instances enable by default, others can enable for a fee
  - required for best performance on GP2 and IO1 volume types

Bad practice: running DB on EC2
  - Why might you want to?
    - very specific need for OS-level access with DB
    - DBROOT for advanced tuning
      - AWS exposes a lot of this stuff tho
    - most reasonable: AWS doesn't support the DB you need to support
  - Why you shouldn't?
    - Admin overhead
      - painful upgrades
      - backups, Disaster Recovery
      - replication, monitoring
    - Single AZ
    - AWS has cool features
    - worse scaling, serverless is better for burst
    - probably not going to optimize it as well as AWS has

Launch specifications
  - can specify in advance AMI, keypair to use, storage, instance type, networking, security groups, userdata, IAM role
  - Launch Configuration
    - older, not editable
    - can only be used with ASGs
  - Launch Template
    - newer features like versioning (still no edits), capacity reservations, placement groups
    - can also be used to launch individual instances

Auto Scaling Groups
  - free, billed for instances obvs
  - Use Launch Configurations or Launch Templates to manage autoscaling and self-healing of EC2 systems
  - all instances in group use the same spec
  - specify min, desired, and max capacity
    - it keeps number of instances at the desired and between the ends
  - Scaling Policy adjusts desired capacity based on cpu load, etc
    - without scaling policy, just manual scaling (desired size is always maintained)
    - configured at VPC level, configuration specifies which subnets to launch instances into
      - instances spread mostly evenly across subnets, best effort
    - policy types
      - Manual
      - Scheduled
      - Dynamic
        - Simple: static adjustment based on alarm
          - "add an instance if total CPU load is > 50%"
        - Stepped: adjustment based on size of alarm breach
          - "add an instance if total CPU load is > 50% or 2 instances if > 80%"
          - conforms to demand curve better
        - Target Tracking: set target cpu, network in/out, ALB request count
          "add instances until total CPU load is 50%"
        - SQS ApproximateNumberOfMessagesVisible
    - Cooldown periods prevent extra cost by smoothing chaotic changes in conditions
      - since there's a minimum billable time when instance is provisioned
  - Self-healing occurs when health check fails and instance is replaced
  - Easy HA trick: 1:1:1 in multiple AZs, instance will be re-provisioned if one AZ fails
  - can put specific instances in standby
  - can attach to ELB target groups so load balancer knows about instances
    - scaling can use richer health checks
      - Simple EC2
        - based on EC2 status, unhealthy includes stopping, shutting down, impaired
      - ELB health check
        - can be application-aware with text pattern matching
      - Custom
        - external system can mark instances unhealthy
      - can set grace period for bootstrapping before instance can fail
      - be careful you don't health check app instances based on DB health or other things that would remove fine instances
  - Scaling Processes (can be suspended or resumed)
    - Launch
    - Terminate
    - AddToLoadBalancer
    - AlarmNotification (repond to CW alarms)
    - AZRebalance (spread across AZs)
    - HealthCheck
    - ReplaceUnhealthy
    - ScheduledActions
  - Lifecycle Hooks
    - Launch and Terminate allow for custom actions to happen at the same time
    - instances pause and continue after a timeout or doing CompleteLifecycleAction
      - can then resume or abandon
      - happens before terminate, after launch
    - supports EventBridge and SNS


============================
Elastic Container Service
============================

managed container based compute service

Configuration
  - Container Definition
    - Tells ECS enough information about the single defined container
      - where your container is (point to a container registry)
      - what ports it will expose

  - Task Definition
    - represents whole application
      - can be made of multiple containers
    - stores
      - container definitions
      - what resources are used by task
      - networking mode
      - compatibility metadata
      - task role
        - temp credentials from role assumed by task
        - best practice for giving ECS containers AWS permissions

  - Service Definition
    - how do we want task to scale, including load balancing
    - how to handle failed tasks
    - can provide High Availability

Clusters
  - where containers run
  - you give ECS an image and it makes a container in a cluster
  - container is in a container registry
    - AWS has one, ECR
  - Tasks and services can be deployed into clusters
  - 2 modes
    - EC2 mode
      - EC2 Management Component
        - manages cluster, instance placement, scheduling
      - Auto Scaling Group manages horizontal scaling of instances
      - instances are provisioned
        - you pay for them even if you launch no containers
      - you have to worry about capacity of your cluster
      - task images are deployed as containers on these host instances
      - you can use spot pricing or reserved instances
    - Fargate mode
      - Serverless, you don't manage the EC2 instances
      - Containers run in Fargate Shared Infrastructure
      - tasks and services are given network interfaces in VPC

Use ECS vs just EC2 instances if you use containers
Use EC2 mode if you have a large workload and price is important (so you can use spot or reserved pricing)
Use Fargate mode if you have a large workload and low overhead is important
Use Fargate mode if you have a burst or periodic workload
  - with EC2 mode you always have a fleet of container hosts running
    - sort of? good tuning could scale down and be as efficient, but there's friction and it's overhead in any case


============================
Elastic Container Registry
============================

ECR
  - hosts container images
  - each registry can have many repositories with many images
  - images can have tags (unique within repository) 
  - Each AWS account has:
    - Public Registry
      - anyone can read, need permissions to write
    - Private Registry
      - need permissions to read or write
  - permissions integrated with IAM
  - Cross-region and cross-account replication available

Image security scanning
  - Basic
  - Enhanced
    - uses Inspector product
      - can scan OS and software packages for issues

Reporting
  - Near-real-time metrics through cloudwatch
  - API events logged to cloudtrail
  - Events generated for EventBridge


============================
Kubernetes
============================

Open-source, cloud-agnostic container orchestrator

Highly available cluster of resources organized to work as one unit

Architecture
  - Control Plane
    - manages scheduling, applications, scaling, and deployment
    - Components
      - API
        - nodes and other elements communicate with this
        - can be horizontally scaled for HA and performance
      - etcd
        - Highly Available key-value store
        - main backing store for cluster
        - kinda works as a queue of pods
      - kube-scheduler
        - identifies pods with no nodes
        - intelligently assigns node to pod
      - clound-controller-manager
        - interacts with AWS, Azure, other cloud platforms
      - kube-controller-manager
        - Node controller - monitors nodes and responds to outages 
        - Job controller - runs pods
        - Endpoint controller - runs pods
        - Service Account & Token controller - Account and API token creation
  - Cluster Nodes
    - VM or physical server
    - functions as worker in the cluster
    - Runs
      - container runtime like docker or containerd
      - kubelet to interact with control plane via kubernetes API
      - kube-proxy or k-proxy
        - coordinates networking with control pane
        - configures rules to allow communication with pods inside or services outside the cluster
          - nodes' OS-level networking
  - Pods
    - smallest unit of computing
    - description of some work to be done plus some metadata about where and how
    - can use multiple containers but one-container-per-pod is common
      - only have multiple if they are tightly coupled
    - ephemeral, should be stateless

  - Jobs
    - ad-hoc process
    - creates pods
    - can retry

  - Services
    - collection of pods, can be thought of as an application

  - Ingress
    - entrypoint for external actor to use service
    - Ingress -> Routing -> Service -> Pods
    - uses Ingress Controller
     - AWS would use a load balancer to control ingress

  - Persistent Storage
    - lives beyond life of an ephemeral pod


============================
Elastic Kubernetes Service
============================

AWS-managed Kubernetes
Can run different ways: on AWS, on AWS Outpost (on-premises mini-AWS), EKS Anywhere, or even open-source EKS distro
Control pane runs across multiple AZs
  - etcd also across multiple AZs
Integration with AWS products: ECR, ELB, IAM, VPC
Persistent storage can be EBS, EFS, FSx

Nodes
  - can be
    - Self-managed
      - EC2 nodes you're billed as usual for and manage
    - Managed Node Groups
      - also EC2 but provisioning is handled by AWS
    - Fargate pods
      - more hands-off
  - Have to consider node type for features like windows, GPU, Local Zones, etc

Networking
  - EKS Control Plane uses AWS-managed VPC 
    - ENIs injected into customer VPC to communicate with it
      - consumption is done through customer vpc via ingress configurations
      - also can use public endpoint


============================
SSM Parameter Store
============================

(simple) Systems Manager Parameter Store
  - storage for config and secrets
  - heirarchy ("folders") and versioning
  - String, StringList, and SecureString
  - SecureString is Cyphertext encrypted by KMS
    - KMS permissions needed for this
    - `aws ssm get-parameters-by-path {path} --with-decryption` gets plaintext


============================
Relational Database Service
============================

Database server as a service
  - can have multiple dbs on RDS server
  - lots of DB options like MariaDB, Microsoft SQL, MySQL
  - Security group is attached and controls what has access
  - no SSH or OS access
    - RDS Custom can give some low level access
  - operates in vpcs in a region
  - can give public ip address in a public subnet, but highly discouraged
  - each instance gets its own dedicatd EBS storage
  - High Availability
    - Multi-AZ Instance Deployment
      - choose one subnet for primary instance and one for standby (High Availability)
        - if not specified, random but in different AZs
      - launches instance in both
      - means we need to add a subnet in each AZ we want to use
      - data replicated to standby synchronously
      - failover can take 1-2 minutes
      - comes with an endpoint (RDS CNAME)
        - always points to primary (even after standby becomes primary due to failover)
      - backups occur from standby instance
      - standby instance can't be read/written to directly
        - can make it primary manually though, good for maintenance
    - Multi-AZ Cluster Deployment
      - 3 instances across 3 AZs
        - Writer and 2 Readers
          - Readers can be read from, unlike standby instance in instance mode
          - Writer can be read or written
      - Writer replicates changes to readers
        - Data is committed when 1 reader 
      - Fast writes to local SSD Storage, then queued to EBS
      - Replication is via transaction logs (faster)
        - allows 35s failover
      - Endpoints
        - Cluster Endpoint points to writer, used for reads writes and admmin
        - Reader Endpoint points to some available reader
        - Instance Endpoints used for testing/troubleshooting
  - Subnet Group
    - list of subnets RDS can use for a given DB
    - a good strategy is one subnet group per RDS deployment
  - Backups
    - stored in managed bucket in S3 (can't see in S3)
      - thus regionally resilient
      - cause IO pause, if using one instance then it could delay performance
    - Snapshots
      - manual
      - All DBs on instance are backed up
      - Don't expire or get deleted with RDS instance
      - like other snapshots, incremental
    - Automated Backups
      - similar, happen during backup window
      - every 5 minutes transaction logs are stored
        - store the actual changes + operations, used w/ snapshot to restore to 5 min granularity
      - Set expiry up to 35 days
        - even if you retain backup after deleting instance, expiry applies. Need to do "Final Snapshot" to keep
      - can replicate cross region
    - Restores
      - creates new instance with new address
      - take time to finish
  - Cost
    - Instance size and type billed per second
    - Multi-AZ costs more
    - EBS Storage cost
    - data transfer per gig
    - snapshot storage is free, extra backup is per GB month
    - licensing if applicable
  -Read Replicas
    - can have 5 per instance
      - can have read-replicas of read-replicas, but it can get laggy
    - replicated to asynchronously
    - can be cross-region
    - good for scaling read load
    - good for adding resilience and recovery time
      - Restores take time, but failing over to a read replica is fast
      - doesn't help with data corruption
  - Security
    - Encryption
      - Can set encryption in transit to mandatory on a per-user basis
      - supports EBS volume encryption with KMS
      - logs, snapshots, etc are encrypted
      - can't remove encryption from host
      - Oracle & M$ support Transparent Data Encryption, handled within the db engine instead of volume
      - Oracle supports CloudHSM, with stronger key controls that trust AWS less
    - Authentication
      - local db user configured to use AWS Auth Token
        - policies on IAM instance role contain mapping between role and local RDS db user
        - this lets role generate an auth token with 15 minute validity that can log into db
    - Authorization
      - RDS permissions are handled with regard to local user
      - can't control RDS permissions through IAM permissions (only admin permissions like creating new RDS instances or connecting in general)
  - RDS Custom
    - fills gap between EC2 db instance and RDS
      - RDS with lower level access
    - available for MS SQL & oracle
    - can use SSH, RDP, Session Manager
    - less managed; you'll see the EC2 instance and EBS volumes
    - to customize, pause first, customize, then resume
    - OS etc is shared responsibility between you and AWS

Aurora
  - Provisioned
    - Cluster
      - single primary instance, 0-15 replicas
        - replicas can be read from, only primary written to
          - write endpoint (primary), read endpoint (load-balanced)
      - instances don't use their own local storage, shared volume instead
        - 128TiB limit
        - 6 storage replicas in 3 AZs 
          - self-healing, heal using other replicas
          - enough redundancy that disk failure doesn't mean restore is needed
        - makes adding replicas more efficient (don't need to bootstrap new storage)
        - billed for what you use, no allocation
          - billed for high water mark, need a new cluster if data use is reduced significantly (!)
            - being phased out
    - Billing
      - no free tier
      - cheaper for all but tiniest RDS usage
      - charged per second of compute, 10min/hr min
      - storage billed GB month, IO cost per request
      - Backup free up to amount stored
    - Restore, Clone, Backtrack
      - Backups work same as RDS
      - Restore creates new cluster
      - Backtrack can do in-place rewinding
      - fast clone available
        - references original db and stores differences
          - not sure why you'd need to make a lot of clones so who cares about speed here but w/e
  -Serverless
    - Much simpler, easier
    - Cluster
      - uses Aurora Capacity Units
      - set a min/max to use based on usage
      - can scale down to 0
      - same 6-volume-replica resilience
    - Entrypoint
      - AWS proxy fleet (invisible to us) sends incoming request to stateless ACU
    - good for infrequently-used applications, new applications (unsure of scale), variable workload, multi-tenant (more scaling comes with more moneys)
    - billed per-second
  - Global Databases
    - secondary cluster in a second region (up to 5)
      - up to 16 read-only replicas
      - one volume replica, ~1s lag behind primary
    - good for global read scaling, Disaster Recovery for region failure
  - Multi-Master
    - multiple writeable (and readable) instances
    - no load balancing, app needs to manage
      - it's not as simple as sticking the instances behind a load balancer bc
        - they use persistent connections
        - this naive approach can only really round-robin servers instead of intelligently route
    - writes propose a change, gets a quorum of nodes to say it's OK
      - simultaneous write to the same row through a different node would be rejected
    - changes also have to be written to in-memory cache of other read/write nodes

RDS Proxy
  - maintains a pool of connections to the DB that it can use on behalf of applications that consume it
  - runs inside a vpc, only accessible inside
  - Why?
    - opening and closing connections uses resources
    - can multiplex - fewer db calls than client calls
    - do we really want every lambda call to open and close a connection?
    - database failover doesn't terminate connection with proxy/application
  - When?
    - we have smaller db instance and too many connections
    - specifically lambda gets a boost

Database Migration Service
  - runs using EC2 replication instance
  - instance uses migration tasks
    - tasks point at source and target DBs
      - one of them must be running in AWS
    - task types
      -  Full Load: one-off migration of all data
      -  Full Load + Change Data Capture: includes ongoing changes
      -  CDC Only: can be useful if you use a different method to do the Full Load 
      - while cdc is going and after full load + cdc catching up and stabilizing, you can switch over
        - near-zero downtime
  - Schema Conversion Tool
    - converts schemas from one version or engine to another
  - Snowball
    - for large multi-TB migrations
    - physical device (!) that you mail to AWS after using conversion tool to dump db onto it
    - then they stick it on S3 and migrate to target from there
      - this counts as a conversion, that's why SCT was used
    - can keep tracking CDC changes


===================
Elastic File System
===================

NFSv4 (Network Filesystem) implementation in AWS
  - can be mounted into a folder on linux (only) instances
  - can be mounted to multiple instances that share the data
  - Private service, runs in VPC
  - EFS launches Mount Targets in AZ in VPC for EC2 instances connect to 
    - for HA put a mount target in each AZ you use
  - General Purpose mode is good for 99% of cases, low latency, general file serving
  - Max I/O good for highly parallel jobs like big data
    - jobs split over many instances accessing EFS at the same time
    - higher latency
  - Bursting mode is like GP2, speed based on storage size
  - Provisioned mode is like IO-1, speed set separate from size
  - Standard and Infrequent Access classes
    - can move between with lifecycle policies


===================
AWS Backup
===================

- Consolidate backups and restores across products and even accounts
- Fully managed
- wide range of products including s3, RDS, aurora, EBS

Backup Plans
  - frequency, window, lifecycle, vault, region copy
Resources
  - What we want to backup
Vaults
  - Where backups are stored
  - Vault lock is a feature that makes it impossible to delete something (earlier than retention expiry)

- Can also do manual backups
- Can do Point-in-time recovery
- When a backup is transitioned to cold storage, it needs to be there for >= 90days


======================
Elastic Load Balancer 
======================

Evolution
  - version 1 (Classic Load Balancer)
    - outdated, always use v2 for new deployments
    - not true layer 7, can do http/https but don't understand them
    - lack features like target groups and rules
    - 1 SSL cert per CLB (SNI not supported)
      - could necessitate many additional CLBs, different domains can't use same
  - version 2
    - Application Load Balancer
      - layer 7, understands http(s)
        - cookies, headers, user location, etc
      - connection does not go directly through from user to layers, new connections made from LB
        - could have security implications
      - rules direct connections which arrive at listener
        - priority order, catch-all is lowest priority
        - rules have conditions like headers, query strings, path, and source ip
      - actions are taken like forward, fixed response, or even some auth like Cognito
    - Network Load Balancer
      - Layer 4, understands TCP, TLS, UDP
        - can't interpret headers, etc.
        - no session stickiness (which uses cookie)
      - faster than ALB
      - only simple ICMP/TCP health checking
      - can have static IPs
        - ALBs can't because the ALB dns name resolves to the nodes' IPs
      - can handle unbroken encryption
    - Gateway Load Balancer
      - enables transparent inspection of requests/responses
        - think a security appliance that filters traffic
          - particularly, a 3rd party one in a separate VPC
      - uses GENEVE tunnel
      - GWLB Endpoint is in Internet Gateway route table
        - points to GWLB, which encapsulates packet and points to tunneled instances
        - tunneled instance does modification/inspection
        - packet goes back to GWLB Endpoint
        - GWLB removes capsule and sends to original destination
      - This all means packet doesn't need source/dest to be rewritten and stuff


Architecture
  - accepts and distributes connections from users
  - abstracts infrastructure for good failover
  - choose between ipv4 or dual stack (ip 4 and 6)
  - choose 1 subnet in each AZ you want to use
    - 2+ is recommended
  - nodes are deployed in those AZs, and the region-level load balancer distributes equally between them
    - an A record DNS name points to those nodes
    - Cross-Zone load balancing means each node is aware of instances across all zones and can distribute cross-zone
      - if nodes could only route to their own zone instances, instances with fewer peers would be slammed
  - choose between internet-facing (nodes given public IPs) or internal
  - need 8 or more free ip addresses in subnet to function
    - to scale up, have more available
  - a load balancer per tier separates the tiers and isolates scaling
    - one big master elb would scale clunkily with the whole system and managing per-tier access would be a pain

Secure connections
  - Bridging
    - default
    - can see http(s) level info
    - connection is terminated on ELB (separate connection goes further between ELB and servers)
      - decrypted on the load balancer
        - needs certificate for domain name because of this
          - AWS has access to this cert, may affect regulation
        - sent further on re-encrypted
          - instances have to have certs and do cryptography, might be expensive
          - maybe not always encrypted to instances?
  - Passthrough (NLB)
    - can only see destination and ports
    - no https decryption done
    - instances still have to have certs and do cryptography
  - Offload
    - like bridging but not reencrypted, just uses http for the instances
      - so, no cert on instances or cryptography
    - downside is plaintext data within AWS

ALB Session Stickiness
  - non-sticky servers need to be stateless, with session handled externally (db or some such thing)
    - otherwise, you'd be logged unpredictably
  - stickiness uses AWSALB cookie to retain (and request) a specific instance
    - not as good, instance can fail or cookie expire (1-7 days, configurable)
      - a different server is then chosen and process repeats
      - load balancer can't distribute load as well
    - kind of a hack it seems?


======================
Lambda
======================

- Function As A Service
- define runtime (python version, etc.) to run code inside
- define amount of memory
  - more memory, more CPU (linear)
- max 50MB zipped or 250MB unzipped
- 512 default /tmp storage
  - can scale this up
- can run for up to 15min
- security done with execution roles (IAM roles)

Networking
  - Public Networking
    - default
    - runs in public AWS space
    - functions can access public AWS services and the public internet
    - best performance because generic networking/hardware can be used
    - can't access vpc without public addressing/security rules
  - VPC Networking
    - runs in a VPC
    - can access allowed things in same VPC
    - can't access external things like public AWS services (DynamoDB) or internet
      - like anything else in a VPC, needs more to do so
        - for internet, NAT Gateway (bc it can't have a public IP) and IGW
        - for public AWS zone, above or VPC endpoint (better)
    - used to need ENI auto-created per execution to talk to lambda service VPC
      - added invocation delay
    - now, this is optimized and an ENI is made per subnet + SG combo instead
      - adds 90 seconds or so one-time setup when creating or modifying

Security
  - Execution roles act like instance roles
  - Resource policy controls what services and accounts can invoke the lambda
    - can't manipulate through UI

Logging
  - Logs from executions go in Cloudwatch Logs 
    - needs execution role permissions
  - Metrics like successes, failures, latency go in Cloudwatch
  - X-Ray integration for distributed tracing of a path through a serverless app

Invocation Methods
  - Synchronous
    - CLI or API or API Gateway invokes function and waits for a response
      - result is given immediately during request
    - errors and retries have to be handled client side
  - Asynchronous
    - typically when AWS services invoke lambdas
    - lambda is responsible for retries
      - configurable between 0 and 2 times
      - failed retries can be sent to dead letter queue
    - function needs to be idempotent
      - guaranteed to run at least once, but network problems, etc. can make them run more than once
    - results can be sent to additional destination
  - Event Source Mapping
    - used on streams and queues that don't support async lambda invocation (DynamoDB streams, SQS, Kinesis, etc.)
      - stuff where polling is involved
    - the ESM polls for source data
    - source data is batched and sent to lambda
    - needs permissions on the Lambda Execution Role to read data batches, unlike other methods

Versions
  - code + configuration
  - immutable, has its own ARN
  - $Latest alias, custom aliases

Startup time
  - Cold start: execution context is created, could take 100s of ms
  - stays warm for an amount of time, context can be used again
  - have to assume cold start, parallel executions create more contexts
  - Provisioned Concurrency feature can keep execution contexts warm for you


============================
EventBridge
============================

- Replacing Cloudwatch Events
- if X happens or at Y time, do Z
  - routing based on either pattern matching events or pattern matching times (cron)
- default event bus is created for account
  - in EventBridge you can create more, in CWE this is all you get
  - gets events from all supported AWS services
  - events are JSON


============================
Simple Notification Service
============================

- Pub/Sub service
- Highly Available, regionally resillient
- Supports SSE
- Public (need to access public endpoints)
- coordinates sending messages
  - 256kb limit

SNS Topics
  - base entity, controls permissions and configuration
  - Publishers send messages to topic
  - Subscribers receive messages
    - Mobile push notifications, email, http endpoints, SQS, Lambda
    - all subscribers of a topic receive messages
      - can filter
    - fan out pattern has multiple queues respond slightly different to a given message (think multiple encodings)
    - some kinds of subscribers support delivery status, enables retries
      - sns handling should be idempotent
  - Topic Policy is like a resource policy, can enable cross-account access


============================
Step Functions
============================

- describe a serverless workflow

State Machine
  - 2 kinds
    - Standard Workflow
      - max duration 1 year
    - Express Workflow
      - for high volume things
      - max duration 5 minutes
  - started via many things (API Gateway, EventBridge, Lambda, Manually)
  - described via Amazon States Language, a JSON format
  - gets permissions from IAM role
  - States are things that occur
    - SUCCEED/FAIL: end of flow
    - WAIT: waits for a period or until a point in time
    - CHOICE: chooses path based on input
    - PARALLEL: does multiple things at the same time
    - MAP: does the same thing to multiple list items
    - TASK: the meat, performs actions with Lambda, SQS, other Step Functions, ECS, more)


============================
API Gateway
============================

- sits between app and integrations (services)
- entry point for apps
- HA, scalable
- lots of features like auth, throttling, CORS, caching, AWS service integration, more
- public
- can connect to AWS or on-prem (or public anywhere) services
- HTTP, REST (with more features than just HTTP), or WebSocket APIs

Authentication
  - can natively validate Cognito tokens from client auth
  - Custom (lambda-based) authentication
    - includes bearer token
    - calls lambda authorizer function to validate
      - this sends back IAM policy and principle identifier
    - lambda evaluates whether to continue or 403

Endpoint Types
  - Edge-Optimized
    - routes to nearest Cloudfront Point Of Presence
  - Regional 
    - simple regional endpoint
    - faster if users are near region (doesn't have to involve CloudFront)
  - Private
    - accessible only inside VPC via Interface Endpoint

Stages
  - prod, dev, etc
  - one deployment per stage
  - can roll back per stage
  - canary deployment for a stage can take a percent of traffic and eventually be promoted

Errors
  - 4XX Client errors
    - 400 generic bad request
    - 403 access denied
    - 429 throttle limit
  - 5XX Server errors
    - 502 bad gateway (backing service returned bad data)
    - 503 major outage
    - 504 integration failure, often timeout (29s)

Caching
  - configured per-stage
    - size between 500MB and 237GB
    - default TTL 5min, max 1 hour
    - can be encrypted


============================
Simple Queue Service
============================

- Public, Fully Managed, Highly Available queue
- 256kb max message size
  - just link to big stuff in s3, etc.
- queue length can trigger lambdas or ASG scaling
- fan out = consumer is SNS topic w/ multiple queues for parallel processing
- usually used with one producer (group) and one consumer (group)
  - used for decoupling tasks, not data ingestion (that's for kinesis)
- billed based on requests (1-10 messages < 64kb total)
- supports KMS SSE
- access controlled by identity policies or queue (resource) policies
- consumers poll queue for messages
  - Short polling consumes a request even if 0 messages
  - Long polling waits waitTimeSeconds if 0 messages for one to come through, then responds
    - best practice
    - up to 20s

Visibility Timeout
  - received messages are hidden for VT
  - consumer expected to explicitly delete message after processing
    - If they don't, message reappears after VT for another consumer to pick up
  - If a message is received too many times without being deleted, they can be added to Dead Letter Queue
    - ReceiveCount, maxReceiveCount
    - retention period is based on original message enqueue time
  - 0-12 hours, default 30s
    - can be changed per-message (probably by the consumer) with changeMessageTimeout

Queue Types
  - Standard
    - order is not guaranteed, best effort
    - at least once delivery, app has to accomodate more than once
    - scales as high as you need
  - FIFO
    - order is guaranteed
    - exactly once delivery
    - must have ".fifo" suffix
    - limited to 3000 messages per second w/ batching, 300 without

Delay Queues
  - postpone delivery of messages
  - messages are invisible (like with VT) for an initial delay 
    - can be set per-message when message is made


============================
Kinesis
============================

- scalable streaming service
- public, HA
- near infinite data rates sent into a stream
- stream is made of multiple shards, each with 1MB ingestion and 2MB consumption
  - more shards more moneys
- multiple consumers can access data
- lambdas can be configured to invoke when data is added to stream
- 24 hour moving window of data
  - can be set up to a year for additional cost
  - this introduces a concept of persistence
- storage included
- data stored using 1MB Kinesis Data Records

Kinesis Firehose
  - allows persistence past rolling window
  - fully serverless, resilient
  - near real time (60 seconds)
  - can transform data on the fly with a lambda (can affect performance)
  - can deliver data to
    - http endpoint
    - splunk
    - redshift (uses intermediate s3 bucket)
    - elasticsearch
    - s3
  - can deliver from kinesis stream or can accept data directly
  - billed by volume

Kinesis Data Analytics
  - processes data realtime using SQL
  - ingests from kinesis data stream or firehose, can also reference data in s3
  - data can be sent to firehose (and beyond), kinesis data streams, lambda
  - for adjusting data from input to output
  - kinda expensive
  - useful for elections, esports, leaderboards, security & response, etc

Kinesis Video Streams
  - ingest video data from phones, drones, or non-video time-serialized radar, thermal, etc
  - can access frame-by-frame or as needed
  - persist and encrypt
  - can't access directly from EBS, S3, or other storage means; only KVS API
  - integration with Rekognition and Connect


==============
Cognito
==============

- Authentication, Authorization, and user management for web apps

User Pools
  - sign in and get a JSON Web Token
  - can't be used to access most AWS Resources (API gateway takes them tho)
  - integrated with 3rd party sign ins like google and fb
  - greater UI customization than Identity Pools

Identity Pool
  - can swap user pool and other external identities for AWS credentials
    - if external, this is identity federation
  - can offer unauthenticated access to services
  - assume IAM roles on behalf of identity
  - needs to be configured to support each external identity provider you want
    - can have single provider be user pool, though

- The thing to do is have people log into User Pools and invisibly use that to log into identity pool.


==============
Glue
==============

- Serverless Extract Transform Load
  - unlike the serverful (EMR Cluster) datapipeline product
- moves and transforms data from source to dest
  - glue jobs can be manual or initiated from evenbridge
- Source stores: S3, RDS, JDBC Compatible DB, DynamoDB
- Source streams: Kinesis, Kafka
- Targets: S3, RDS, JDBC

Data Catalog
  - persistent metadata about data sources in region
    - like a database of schemas
  - one catalog per region per account
  - used by Athena, Redshift Spectrum, EMR, Lake Formation
  - avoids data silos by improving visibility of data structures
  

==============
MQ
==============

- queues and topics (one to one or one to many)
- open source message broker, based on Apache ActiveMQ
- for migrating existing queues and topics 
- related keywords: JMS API, AMQP, MQTT, OpenWire, STOMP
- not public, private networking required
- no native AWS integrations


==============
AppFlow
==============

- managed integration service
- exchange data between applications (connectors) using flows
  - source and destinations
- public endpoints, can use PrivateLink
- Custom Connector SDK lets you build connector, popular apps already supported

Connection
  - stores config & credentials to access application
  - configures source mapping and transform


==============
CloudFront
==============

Origin
  - location of content
  - multiple origins can form origin group for HA
    - backup origin used if primary fails
  - classifications
    - S3
      - static website hosting enables different features from disabled
      - can set to be only accessible from CF distribution
      - viewer and origin side protocols must match (https for both, etc) 
      - can have custom headers
      - real-time kinesis logging possible
      - can restrict viewer access to require signed urls or signed cookies using trusted signer or trusted key group
      - default root object usually set to index.html
    - AWS Media Store Container Endpoint
      - exists
    - Everything else (custom origins, ec2 for instance)
      - can choose protocol and SSL protocol version, 
      - can choose port
      - can use custom headers to tag data as coming from CF
        - could also use ip ranges of CF (they are public but change though)

Origin Access Identity
  - used to set origin to be only accessible from CF distribution
    - S3 doesn't have a concept of "allow from a CF dist", needs an identity, managed here with Origin Access Control or legacy Origin Access Identity
    - this is used in Bucket Policy
  - only works for non-static-website-hosting S3 origins
  - Cloudfront becomes OAI, able to use in bucket policy
  - often the only consumer of origin

Distribution
  - base unit of CF configuration
  - gets deployed to CF network
  - choose security policy (TLS version), HTTP versions, certificates

Behaviors
  - subconfigurations of Distributions
  - path pattern matching for different configurations
  - choose http methods, whether to accept HTTP
  - associate lambda at edge functions

Private Behaviors
  - require signed url or cookie
  - multiple behaviors means you can reroute private route to a login page, for instance
  - old way: CloudFront Key is made by an Account Root User
    - key added to account, account is made a trusted signer
  - new way: create trusted key groups and make those the trusted signer
    - don't need acct root user to make
  - Signed URLs
    - provide access to one object
    - useful if client doesn't support cookies
  - Signed Cookies
    - can provide access to groups of files, all of a type
    - can control URL used

Edge Location
  - local cache for consumers to access
  - in more diverse locations than Regions, which are in major cities
  - not regions, can't deploy ec2 into, for instance

Regional Edge Cache
  - larger than edge location, larger cache
  - another cache layer, more like regions in terms of location + quantity
    - so if your server is only in one region, the benefit is large elsewhere
  - for infrequent access where smaller edge locations cache would miss
    - edge misses fall back to RECs, misses then fall back to origin
  - supports Amazon Certificate Manager for https
  - for downloads only, not uploads

TTL
  - default TTL is 24h
  - on cache expiry, request goes to origin and can receive 304 not modified
    - TTL is reset
  - origin can set per-object TTL with headers. doesn't override min/max on behavior
    - Cache-Control max-age
    - Cache-Control s-maxage (same as above)
    - Expires

Invalidations
  - expires everything matching some path pattern on a distribution
  - takes time to do
  - costs money (!)
  - versioned filenames (thing_v1.jpg) are an alternative
    - bypasses browser cache
    - better logging
    - but app has to be updated

SSL/TLS
  - CNAME record created with distribution blahblahblah.cloudfront.net
  - Alternate Domain Names are more CNAME records you can specify for custom domains
    - point to CF with DNS provider
    - need to verify ownership by uploading cert
    - you can't just point DNS to default CNAME because it couldn't prove to browser that it's allowed to serve https content for alternate domain
  - both browser -> CF and CF -> origin need public certs for https
    - origin can be set up to use http though
    - s3 origin handles it for you
    - ALB origin can use ACM
    - EC2 origin needs external generated cert

Server Name Identification
  - historically, 1 https site per ip
    - this is because multiple sites could only be differentiated at http layer 7, after connection has already been made
      - appropriate cert couldn't be given when needed
    - SNI is an extension to TLS that allows host specification at lower layer
      - Very old browsers don't support it, so CF has dedicated IP for extra money (600 bucks a month per distribution!)

Lambda@Edge
  - adjust data between viewer and origin
  - only python and node are supported
  - run in public zone
  - lambda layers not supported
  - different size and time limits from normal lambdas
  - can run at 4 lifecycle hooks: viewer request, origin request, origin response, viewer response
    - viewer side limits are more strict than origin side
  - use cases
    - A/B testing: modify viewer request to hit one of multiple URLs
    - origin migration, high DPI versions, country specific content: modify origin request


============================
AWS Certificate Manager
============================

- regional
  - certs can't leave region
  - need cert in ACM in each region you use ALB
  - ALWAYS USE us-east-1 FOR CLOUDFRONT CERTS
    - this gets replicated to edge locations
- can act as Certificate Authority, either public for internet browsers or private for internal consumers
  - Private required client to trust CA (public amazon CA is trusted by most)
- can import or generate certificates
  - generated ones can auto-renew
- only works with ALBs and CloudFront (maybe some other edge things)
  - not EC2! That direct connection to instance can't be managed by Amazon


=======================
Global Accelerator
=======================

- makes things faster for global users (reduces hops)
- Example
  - starts with 2 anycast IP Addresses
    - as opposed to the Unicast IP addresses we normally see
    - allow a single ip to be in multiple locations
    - routing moves traffic to closest location
  - these 2 ip adresses map to 3 GA edge location (there's overlap)
  - any of the 3 can service requests for either IP, closest does
  - from those edge locations, request does the rest of its stuff
    - path to the destination (some AWS service) is AWS-managed and faster
  - (why doesn't this example use 1 ip address and 2 servers)
- unlike cloudfront, can handle TCP/UDP (non-http)


============================
Border Gateway Protocol
============================

- coordinates traffic between black box Autonomous Systems
  - this is e(xternal)BGP, there's also iBGP
- operates on TCP/179 (reliable with error correction)

Autonomous System
  - collection of routers controlled by one entity
  - one thing as far as BGP is concerned, big abstraction
  - assigned a number by IANA
    - originally 16bit, now there are 32bit ASNs
    - last hundred or so are reserved for private use internal to networks
  - connections have to be set up manually
  - learns about network topology and shares that info with its peers
    - this lets core internet players share best shortest to a destination --ASPATH
      - also aware of longer paths so it's resilient
      - shortest doesn't mean best, some could be slow
        - ASPATH prepending lets you manually override this
          - kinda works like css specificity trick, append dupe ASNs to path in route table: 202,202,202,i (i at end means route was originated locally)
            - who cares how it works tho lol
 

============================
Site-to-Site VPN
============================

- logical connection between VPC and on-prem network
- encrypted using IPSEC
- mostly running over public internet
- HA
- less than an hour to provision

- Virtual Private Gateway
  - can be target in route tables, associated with a single VPC
  - has 2 physical endpoints in different AZs
    - these both connect to Customer Gateway with VPN Tunnel for HA (on amazon side)

- Customer Gateway
  - refers to an on-premises physical router the VPN connects to
  - also used to refer to the aws config that represents it
  - for full HA, 2 of these connect to V(P)GW

- VPN
  - the tunnel connecting CGW with VGW through the VGW endpoints
  - 2 kinds
    - Static
      - manually configure on on-prem and AWS with info about other's addresses
    - Dynamic
      - needed for multi-connection failover and load balancing, and Direct Connect
      - once connection is made via BGP, internal networking can change and tell the other side
        - route propagation means the routes that the VGW learns also get added to the VPC route table
      - needs BGP support
  - Limitations
    - 1.25Gb/s speed cap
      - might be practically lower bc of customer router limitations and encryption overhead
    - latency and variability of public internet (number of hops)
  - Billing
    - hourly, data out
    - external on-prem data rates apply
  - can be used as a backup for Direct Connect and other technologies


============================
Direct Connect
============================

- physical connection between business -> DX location -> AWS Region
- 1, 10, or 100 Gbps standard
  - can be smaller if using 3rd party that shares ports between clients
- physical port at DX location
  - it connects to something else at location, maybe your own servers or maybe a comms provider
- highest hybrid networking speed
- long provisioning time of laying cables
- can access vpcs and public AWS, no public internet access
- not encrypted
- Billing
  - hourly cost for port and outbound data

Resilience
  - by default, single cable from your rented DX port to whatever. not HA!
  - also "last mile" connection to on-prem environment not HA
  - improve by getting 2 DX ports, they'll be on different AWS DX routers
    - connect each to a different customer DX router, connected to a different on-prem router
    - but
      - the 2 connections to on-prem could be the same cable path
      - the buildings could collapse
    - so
      - 2 customer premises buildings
      - 2 DX locations
    - can go crazy and do 2 routers at 2 DX locations

Virtual Interface
  - virtual point-to-point ethernet link between DX location and AWS
  - 3 kinds
    - Private
      - connects to things in private VPC
    - Public
      - connects to public AWS services
      - can connect to vpc using vpn for encrypted connection
    - Transit
      - simplifies on-prem to AWS, and VPC to VPC networking
        - nontransitive VPC peering and HA redundant vpns create a lot of connections to manage and scales badly
      - account-level endpoint that can route traffic to VPCs
        - specify subnet in each relevant VPC to deploy VPC Attachment, similar to interface endpoints
      - can even attach TGs to TGs in peered VPCs in other accounts
      - supports Resource Access Manager cross-account access
      - needs route tables pointing to it


============================
Storage Gateway
============================

- Runs either as standalone hardware or virtual machine (more common) on-premises
- presents storage using iSCSI, NFS, or SMB
- connects to EBS, S3, and Glacier
- useful for migration, running out of on-prem storage, DR, legacy backup replacement

Volume Gateway
  - 2 modes
    - Stored Mode
      - everything is stored on-prem locally with SG VM
      - Upload buffer
        - everything written to disk is copied here and then copied async over network to AWS storage
      - good for full volume backups to ebs snapshots
        - these snapshots are great for DR, can be recovered 
      - doesn't extend data capacity, only for copies
    - Cached Mode
      - data primarily stored on AWS
        - special hidden AWS-managed S3 bucket
      - local cache for efficiency, caches frequently accessed data
      - infrequently accessed things can be slow
      - good for data center extension into AWS

Tape Gateway
  - replaces on-prem tape library
  - Virtual Tape Library presents as a physical tape library via iSCSI
    - S3 normal used for library
      - no robot arms :(
    - Glacier used for Shelf
      - anywhere not in library, where tapes are stored not in-use
    - can eject tapes from library to shelf

File Gateway
  - access s3 files in normal filesystem
  - individual file-level vs overall block volume level
  - Mount points in NFS and SMB
  - multiple on-prem Fileshares connect to a bucket, which creates a Bucket Link
  - no file locking, best to have only one writing fileshare and others be read-only


============================
Snowball
============================

- move large amount of data to or from AWS

Snowball
  - physical device
  - each device is 50 or 80TB
  - overall effective in the range of transferring 10TB-10PB total
  - Data encryption uses KMS
  - needs networking

Snowball Edge
  - like above but
  - comes with compute
  - higher capacity
  - faster max networking speed
  - has variants: Storage Optimized (with ec2), Compute Optimized, Compute with GPU

Snowmobile
  - portable data center on a truck
  - good for > 10PB data
  - impractical for multiple locations
  - up to 100PB per truck


============================
Directory Service
============================

Directories
  - Stores objects (Users, Groups, Computers, Servers, File Shares) with a structure (tree)
  - Common in Windows environments
  - can sign into multiple devices with the same un/pw (think a computer lab)
    - centralized management of assets
  - some providers are Microsoft Active Directory Domain Services, SAMBA (open source alternative)

Directory Service
  - AWS-managed, like RDS is for DBs
  - Runs in VPC
  - HA by deploying into multiple AZs
  - windows EC2 instances can join directory
  - some services like Amazon Workspaces require directory
  - can be
    - Simple AD (standalone)
      - SAMBA 4, open-source
      - supports up to 500 in small mode or 5000 in large mode
    - AWS Managed Microsoft AD
      - still has direct presence in AWS
      - can establish trust with on-prem
        - needs to occur over private networking (VPN or Direct Connect)
        - users can cross-access directory objects but they aren't synced
        - AWS can still access local (on AWS) directory if VPN fails, trust has been established (which does local mean here)
        - supports advanced MSAD features like schemas
    - AD Connector
      - proxy mode to use on-prem directory with services that need directory service
      - good if you already have directory and want to use like one AWS service that needs DS
      - services will fail if VPN fails


============================
Transfer Family
============================

- Transfer to/from S3 and EFS via provisioned transfer servers
- Multi AZ, Resiliant, scalable
- Good if you need to use protocols other than native S3 and EFS.
  - FTP - unencrypted
  - FTPS - FTP with TLS encryption
  - SFTP - transfer over SSH
  - Applicability Statement 2 - Structured Business to Business Data
- Identities - Service managed, Directory Service, Custom (lambda, APIGW)
  - no service-managed for FTP and FTPS
- Supports Managed File Transfer Workflows - serverless file workflow engine

Endpoints
  - Public
    - nothing to configure
    - only supports SFTP
    - dynamic IP managed by AWS, so use DNS
    - can't control access via IP lists
  - VPC w/ Internet
    - inside VPC
    - Supports SFTP, FTPS, AS2
    - Supports NACLs and SGs
    - can access via VPN/DX
    - can access via internet using the Elastic public IP
    - Static private IP
  - VPC Internal Only
    - like above, but also supports FTP
    - no Elastic public IP

- Billed for provisioned server per hour + data transferred


============================
DataSync
============================

- Huge scale data transfer to and from AWS
- Good for migrations, archival, DR/BC
- keeps metadata (permissions, timestamps)
- built-in data validation
- Scalable, 10Gbps per agent (~100TB/day)
- Supports bandwidth limiters to avoid "link saturation" (not be a network speed hog)
- Supports incremental and scheduled transfers
- Supports compression and encryption
- Integrates with S3, EFS, FSx (for windows)
  - can even do service-to-service transfers like cross-region EFS-EFS
- Pay as you go per GB

Architecture
  - Tasks define what is being synced, how fast, src and dest (both are Locations)
    - NFS and SMB are considered Locations
  - DataSync Agent runs on VM or the like and connects to NAS/SAN storage via NFS or SMB, communicates with DataSync endpoint


============================
FSx for Lustre
============================

Lustre itself
  - File system designed for High Performance Compute on Linux clients
  - FSx is the AWS-managed service using it
  - Supports POSIX style permissions
  - Metadata like filenames, timestamps are stored on the Lustre FS's single Metadata Server Target
  - data is split over multiple Object Storage Targets
    - 1.17TiB each

FSx for Lustre
  - Good for Big Data, Machine Learning, Financial Modelling
  - 100s of GBps throughput and sub-millisecond latency
  - 2 work modes
    - Scratch
      - highest short term performance
      - no replication or HA
      - hardware failure loses some data
        - larger file system means more points of failure
    - Persistent
      - longer-term
      - single AZ HA
      - self-healing
  - Accessible via private networking
  - When creating the file system, can associate with S3 bucket
    - Files are visible but lazy-loaded into FSx
    - data can be written back to bucket with hsm_archive command (manual or triggered)
    - Processing happens on the Lustre FS, separate from this bucket!
  - baseline performance based on size
    - Scratch: 200MB/s per TiB of storage
    - Persistent: 50, 100, or 200MB/s per TiB of storage (pay more for more)
    - earn credits for going below baseline, consume credits for going above
      - can burst using credits up to 1300MB/s per TiB
  - min 1.2TiB, additional increments of 2.4TiB
  - can back up to S3 manually or automatic with 0-35 days retention

Architecture
  - EC2 clients have Lustre software installed to read and interact with managed FSx for Lustre
  - FSx for Lustre deploys Lustre file servers that handle storage requests and have caching
    - parallel processing goes brrr
  - one Instance Endpoint ENI deployed in VPC to talk to FSxfL
  - writes and cache misses are limited by storage IOps
  - cache hits are limited by network speeds


============================
FSx for Windows File Server
============================

- Native Windows Fileshares
- Integrates with windows environments
  - Directory Service AD or Self-managed AD (Active Directory)
- Single or Multi-AZ
  - Single mode is still HA against normal hardware failure
- Scheduled and on-demand backups
- Accessible via Private Networking

- Native Windows Filesystem means it supoprts features like deduplication, Distributed File System, KMS at rest and enforced encryption in transit
- supports volume shadow copies (file level versioning)
- accessed via SMB
- performant
- windows permissions model


============================
Secrets Manager
============================

- Shares functionality with Parameter Store
- designed for priviledged data
- uses lambda to support auto-rotation
  - has versioning
- directly integrates with RDS and other services
- encrypted at rest
- uses IAM


=========================
AWS Config
=========================

- Monitors and records changes to resources over time
- good for audits and compliance
- doesn't prevent changes
- regional, but can be configured for cross-region or cross-account
- can generate events and SNS notifications
  - Config rules evaluate whether resource is compliant
  - changes in compliance could go through EventBridge > Lambda to fix problem
    - can also trigger SSM runbook instead of lambda
- stores change log in S3 Config Bucket
  - changes create Configuration Items


=========================
AWS Shield
=========================

- DDOS protection
- 2 modes
  - Standard
    - Free
      - automatically enabled at the network perimeter (edge or VPC)
      - best protection with R53, CloudFront, and Global Accelerator
  - Advanced
    - $3000 per month per AWS Organization
    - 1yr minimum
    - data out is extra charge
    - protects R53, CloudFront, and Global Accelerator, and additionally EIPs (like with EC2), ALBs, CLBs, NLBs
    - Not automatic, must configure in Shield Advanced or AWS Firewall Manager Shield Advanced Policy
    - cost protection for things that should be but aren't mitigated
      - example: ec2 scales uncontrollably due to attack, you get reimbursed
    - humans warn you if there's a potential attack (Shield Response Team)
      - You can log tickets with SRT for Proactive Engagement
    - Integrates with Web Application Firewall
      - basic WAF fees are included with Shield Advanced
      - can inject temporary WAF rules to block IPs for instance
    - can detect some L7 anomalies like header anomalies, geographic anomalies, abnormal URL access like many /admin calls
    - real time attack visibility
    - app-specific health checks are used by proactive engagement team
      - no checks means no proactive engagement
    - Protection Groups
      - Simplify managemenyt
      - can help spot multi-vector attacks


=========================
CloudHSM
=========================

- Similar to KMS, creates manages and secures cryptographic material
- true single-tenant Hardware Security Module
  - as opposed to isolated but somewhat shared KMS infrastructure
- aws provision it, but it's in tamper-resistant hardware that you control
- FIPS 140-2 Level 3 compliant (KMS is L2)
- less AWS-integrated by design
  - instead access with industry standard APIs
    - JCE (Java), CNG (M$), PKCS#11
- KMS can use "Custom Keystore" with CHSM integration
- multiple in AZs can be configured as a cluster for HA 
  - stuff is replicated
- Operates from AWS-managed VPC
- injects ENIs into your VPCs
- EC2 software-wise, install AWS CloudHSM Client
- can't do stuff like S3 SSE because AWS would need to use it. Client-side encryption only
- can offload SSL/TLS processing efficiently
- can power Oracle Transparent Data Encryption for DBs
- good for super strong regulations
- good for stuff that needs industry standard, non AWS stuff
- can protect private keys for a certificate authority


============================
Web Application Firewawll
============================

- Compatible with certain products (CF, ALB, APIGW, AppSync)
- Web Access Control List is configuration unit
  - These are what you associate with CF Distribution, say
    - can take time depending on associated service
      - faster to edit an already-associated one
  - created in region unless using global service like CF
    - can't move a regional one to a global one and vice versa
  - default action is either allow or block
  - Web ACL Capacity Units
    - based on complexity of rules
    - default 1500 max
    - can be increased with support ticket
- can block known botnets
- Has Rules in optional Rule Groups
  - Rules have
    - Type
      - Regular (did something happen)
      - Rate-based (did something happen a lot)
    - Statement
      - WHAT and COUNT, depending on Type
        - what can be header, cookies, query, ..., body (8192 bytes)
      - rule can have more than one statement, joind with AND, OR, NOT
    - Action
      - BLOCK, ALLOW (not for rate-based), COUNT, CAPTCHA
      - Custom Response for BLOCKed things, Custom header for ALLOW with x-amzn-waf-
      - Label internal to WAF, can be used to create multi stage rules
  - groups can be referenced by many WACLs
  - define upfront Capacity Units
- Has Managed Rule Groups such as Bot Control group for heuristically blocking bots
  - mostly free, some like Bot Control have fees
- eventbridge can inject ip list rules when ip list changes
- outputs logs (of all requests) to CW Logs, S3 (5min delay), or firehose
  - these can generate events that create more rules if badness is detected
- $5 a month, can be reused
- $1 per rule
- $.60 per million requests
- Bot Control: $10 a month, $1 per million requests
- Captcha: $.40 per 1000 attempts
- Account takeover/Fraud Control: $10/m $1/k login attempts


============================
GuardDuty
============================

- Continuous security monitoring service
- Analyzes supported data services
- uses AI & intelligence feeds
- identifies patterns of what happens normally and detects unexpected and unauthorized activity
- creates notifications or events
- supports multiple (MASTER/MEMBER) accounts
  - MASTER is single location for managing


============================
Inspector
============================

- Scans EC2 instances, instance OS, and containers for vulnerabilities and non-best-practices
- runs assessments that last between 15min - 25h
- provides report of findings
- Rules packages determine what's checked, 2 main types
  - Network Assessment
    - doesn't need agent, though one can help
    - example: Network Reachability (Network Assessment)
      - checks reachability end-to-end including load balancers, IGw, VPCs, SGs, VPC Peers, more
      - RecognizedPortWithListener, RecognizedPortNoListener
      - RecognizedPortNoAgent
        - agent required to tell if there's a listener
      - UnrecognizedPortWithListener
  - Network & Host Assessment
    - needs agent
    - examples
      - Common Vulnerabilities and Exposures
      - Center for Internet Security Benchmarks
      - Security Best Practices (Amazon's opinions)


============================
Macie
============================

- Data security and privacy service
- can monitor and protect data in S3
- findings integrate with Security Hub or might publish event to EventBridge
  - identifies "bad" policies like public buckets in addition to sensitive data
- centrally managed via Org or Macie Account Inviting
- Data Discovery Job
  - automatically discovers Personally Identifiable Information, Financial Data, etc
  - runs on schedule
  - Managed and Custom Identifiers; managed might be Machine Learning, custom might be regex
    - for regex custom identifier, refiners:
      - can specify that certain key words need to be x characters away from match
      - can specify ignore patterns


============================
CloudFormation
============================

- Template
  - YAML or JSON
  - contains logical resources: describe WHAT you want to create
  - templates create stacks of physical resources
    - a template can create 1 stack or many, maybe 20 in each region, sky's the limit
  - if template is changed, stack is changed
  - once physical resource is create-complete, other logical resources can reference physical resource things like ID
- Outputs
  - optional section in template
  - declare values that you can access via CLI or UI, or via a parent stack when using nesting
  - can add description to each value
- Mappings
  - maps keys to values, allowing lookup
  - uses !FindInMap to access
  - can have subkey
- Template Parameters
  - accept input when stack is created or updated
  - can be referenced inside logical parameters
  - can have defaults, allowed values, values of certain lengths, patterns, type (required, including AWS-specific), and NoEcho for passwords
- Pseudo Parameters
  - injected by AWS
  - examples: AWS::Region, AWS::StackId
- Conditions
  - evaluate to TRUE or FALSE
  - processed before physical resources are created
  - uses AND IF NOT EQUALS OR
  - associated with a resource that may or may not be created, depending on evaluation
    - conditions have names, and resources can reference them by name in their own "Conditions" property
- DependsOn
  - CF tries to be efficient
    - do things in parallel
    - create dependency tree
      - references help create this: if an instance references a subnet, it knows it needs a subnet first
  - DependsOn lets you explicitly define dependencies if needed
    - one common example is creating and associating to a subnet an Elastic IP
      - needs IGW attached to the subnet it attaches to
      - only references subnet, so dependency tree is incomplete because IGW might not be created and attached yet
        - after all, creating a subnet doesn't require an IGW








































