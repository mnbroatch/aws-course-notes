==============
General
==============


Storage
  - Architecture
    - Direct
      - on the physical host
      - fast but ephemeral, goes away when instance moves host, fails, etc.
      - instance store
    - Network
      - highly resilient, survives if host fails, persistent
      - EBS
  - Structure
    - Block Storage
      - Presented to OS as collection of blocks, no structure
        - just a colleciton of uniquely addressable blocks
      - file system is created on top of it
      - mountable, bootable
        - most EC2 instances boot from one
    - File Storage
      - presented as file share
      - file system is already built in
      - not bootable, OS only has high level access to files, not low level access to raw data
    - Object Storage
      - abstract
      - has metadata
      - flat key/object
      - scales well
      - not mountable or bootable
  - Performance
    - IO (block) size
    - IOps
    - Throughput: IO * IOps
      - unless there's a throughput cap
      - sometimes changing IO changes the IOps also

Virtualization
  - running more than one operating system at a time on a piece of hardware
  - Apps can't directly access hardware, only OS can because it runs in privileged mode, not user mode
    - but, only one thing could historically be privileged
      - 2 early solutions, had to be done in software
        - Emulated virtualization
          - guest OSs run in virtual machine
            - has allocation of cpu, etc.
          - host OS had a "hypervisor"
            - what the host thought was the hardware was emulated
            - hardware calls are sent to the hypervisor, which interfaces with the real hardware using binary translation
          - performance is slow
        - Paravirtualization
          - still has hypervisor
          - virtual machines run modified OSs (probably linux)
            - know about and make calls directly to the hypervisor instead of having hypervisor translate fake hardware calls
      - modern solution: Hardware Assisted Virtualization
        - CPU knows about virtualization, traps hardware instructions and does them
          - faster because CPU just does it faster
            - it has special circuits and stuff
          - Hypervisor still manages, but doesn't do the nitty gritty
            - sets up VMs, allocates resources, handles edge cases that CPU can't
      - moderner solution: Single Route IO Virtualization (SR-IOV)
        - hardware is virtualization-aware
          - a network card, say, can split itself into many mini cards, and those can be allocated to guest OSs with no translation
        - in EC2, this is called "Enhanced Networking"

Containers
  - virtualization still has issues
    - OS is duplicated and can take up a bunch of resources
      - a waste if multiple virtual machines use same OS (which is common)
  - Instead of hypervisor and multiple OSs, there is a host OS and Container Engine
  - instead of VM, we use container
    - still isolated, has its own file system
    - can run child processes inside
  - Portable and consistent, can run wherever there's a compatible host OS
  - can expose ports
  - Docker is the most popular container engine
    - Starts with Dockerfile that creates image
      - each line creates new file system layer in the createdimage
        - each layer is stored as diff
        - layers are read only
      - first line is base image to extend, or from "scratch" (literal string FROM scratch)
      - entrypoint is what initially runs
      - last layer is special read/write layer where stuff happens during container running
      - sharing the read-only layers is fast and cool
  - Container registry
    - can upload and download container images
    - can be public or private

Firewalls
  - 2 kinds
    - Stateless
      - focuses on inbound and outbound
      - for https, say, client uses random port to connect to server's known 443 port.
        - "connection" is 2 requests: request and response
        - so, 2 rules needed: inbound and outbound, inverse of each other
          - server probably has to allow inbound traffic from all random ports on 443
          - from client's perspective, request is outgoing and response is incoming
          - vice versa for server
    - Stateful
      - focuses on request and response
      - smart enough to know that if request is allowed, response should be
        - knows that a given response is related to request, so only request rule is necessary
        - automatically opens response port when it makes a request

Shared Responsibility Model
  - what does user control vs AWS
    - On Premises -> Datacenter Hosted -> Infrastructure as a Service -> Platform as a Service -> Software as a Service
  - User is responsible for security ON the cloud
    - client-side stuff, operating system, firewall, file system, data, encryption, etc.
  - AWS is responsible for security OF the cloud
    - AWS software, infrastructure, storage, etc.

Availability
  - High Availability
    - Ensure performance for higher than normal period
      - period can be big or bigger
      - If something fails, it can quickly be replaced
      - replacement might make users log in again, for instance, but that's OK.

  - Fault Tolerance
    - ensure system keeps working through failure
      - Like if above example didn't make user log in again. Redundancy is a good tool here.

  - Disaster Recovery
    - What to do if HA/FT fail
    - pre-planned and documented processes
    - backups stored elsewhere from main storage

Public vs Private Services
  - Refers to networking and where you can access from
  - Private services can't be accessed from outside VPC (not connected to VPC)
  - Private Zone is like a home network, things inside can talk to stuff that things outside can't
    - VPCs are inside Private Zone
  - "AWS Public" zone is between public internet and VPC
    - Not inside public internet, but connected to it.
    - S3 for instance is here
    - You use public internet to get to AWS Public zone, which routes your requests internally
  - VPN can connect private-to-private network directly to private zone (edge case, eg on premises)
  - Internet Gateway can allow private zone stuff to access internet, or go directly to AWS Public zone for s3 queries, etc.
    - EC2 instance can have a public IP for instance.
      - Projects EC2 into Amazon Public Zone

Global Infrastructure
  - Regions -> Edge Locations
    - Edge locations are good for netflix, for instance. Better performance.
  - Regions give
    - Isolated fault domain
    - Geopolitical / legal separation
      - Different laws might apply
    - control; you might expand infra into a different region for better performance
  - Availability Zones
    - isolated hardware
    - architecture can distribute between them for region resiliance

Horizontal vs Vertical Scaling
  - Vertical
    - just use a bigger instance, dummy
    - max instance size is a limit
    - expensive
    - requires reboot (disruptive)
    - easy, no app changes needed
    - monoliths work
  - Horizontal
    - cheaper probably
    - no limit
    - granular, can add just a tiny bit more if you want
    - session management requires
      - application support
      - or off-host sessions
        - instances are stateless


==============
VPC
==============

- Virtual network
- Created within one region and one account
- by default, private and isolated from public AWS zone and other VPCs
- VPC CIDR defines range of IPs in use
- Subdivisions of VPC (Subnets) are each located in 1 AZ


Default VPC is a pre-configured thing that we don't use
  - only one per region per account
  - CIDR is 172.31.0.0/16
  - One subnet per AZ in region, /20 size
    - plenty of spare room
  - Internet gateway, Security Group, NACL created
  - Anything placed in default subnet gets a public IP. Different from Custom VPC
  - can be deleted / recreated
    - Some things expect it to exist tho

Design considerations
  - are there networks we have to avoid
    - other vpcs, cloud, on-premises, partners/vendors
  - what size range
    - vpcs can be between /28 (16 ips) and /16 (65536 ips)
  - how many ranges
    - how many subnets
      - how many AZs?
        - three per region seems good, plus a buffer for a total of 4 subnets due to AZs
      - how many tiers? a common pattern is
        - public web tier
        - private with gateway application tier
        - private with no gateway database tier
        - let's add a spare
      - So, 16 subnets per account. if starting with /16 vpc, this would be /20 each subnet.
        - starting with /18 would make /22 subnets
      - how many accounts?
        - Prod, dev, general, spare
      - we need 48 ip ranges
  - the class lost me here re: the A4L example. it started talking about having 4 vpcs per region/account but also all AZs and all Tiers as subnets in 1 vpc
    - not sure where the 4vpcs come from. just being big?
  - don't forget we need a vpc per region
  - try to predict the future (?)
    - is there more to that than "leave spare room"

Custom VPC
  - isolated; nothing in or out without explicit permissions
  - hybrid networking: on-premises + cloud
  - 2 hardware tenancy setups
    - Default
      - shared hardware
      - can choose per-resource to have dedicated hardware
    - Dedicated
      - can no longer choose shared hardware
      - more expensive
  - everything gets private IP, public ips are given when we need them
  - one primary IP4 CIDR /16 block
    - up to 5 (more with support) secondary cidr blocks
      - these are if you run out of space, essentially
  - one optional IP6 /56 CIDR block
    - can't choose block, can supply ones you already own
  - DNS
    - provided by Route53
    - dns ip is base ip + 2
    - VPC-level options
      - EnableDnsHostnames
        - because ips can be dynamic and this will resolve correctly
          - why did I not know this while working on reply-guy?
      - EnableDnsSupport
        - enables or disables DNS resolution

Subnets
  - AZ Resilient, inside 1 AZ
  - ip4 cidr is subset of vpc cider
    - optional ip6 /64 range, 1/256th of the VPC ip6 range
  - subnets in a vpc can't overlap
  - by default, can freely communicate between subnets within a vpc
  - reserved addresses
    - first is vpc itself, network address
    - first + 1 is vpc router
    - first + 2 is dns address, helps use hostnames internally and externally
    - first + 3 is reserved for future use
    - last is Network Broadcast address which talks to every host
      - broadcasting is not actually useable in VPC
  - for every vpc there is a DHCP option set
    - DHCP assigns IP addresses
    - can't edit options, can create new option set instead
    - applies to all subnets in vpc
    - options
      - Do we give resources a public ipv4 address by default?
      - Do we give resources a ipv6 address?

VPC Router
  - every VPC has one
  - network + 1 ip
  - Highly available, runs in all the AZs the VPC uses
  - routes traffic between subnets
  - controlled by Route Tables
    - can be associated with many subnets
    - each subnet has one
      - default one is the VPC-level Main Route Table
    - list of routes, which have destinations which can be ip ranges (or /32 for 1 ip address)
      - if multiple routes match incoming packet, most specific applies
      - each route has a "target", either "local" or an AWS Gateway
        - local means within VPC
    - each route table has the "local route"
      - matches VPC CIDR range with target "local"
      - if enabled, also has ipv6 local route
      - LOCAL ROUTE ALWAYS TAKES PRIORITY
    - then what are non-local routes good for? some examples
      - traffic out to internet
        - instead of default internet gateway, could route to vpn
      - necessary for cross-vpc communication
      - necessary for private subnet to send traffic out to internet


Internet Gateway
  - Region Resillient
  - IGW can have 1 or 0 VPCs and vice versa
  - runs in AWS Public zone, coordinates traffic to/from internet or public zone
  - steps
    - create IGW
    - attach to VPC
      - now we can use it in route tables
    - create and attach custom route table to subnet
      - create default routes 0.0.0.0/0 and ::/0 to the IGW
  - whenever a resource has a public ip, the internet gateway actually maintains the map to the private IP
    - ONLY V4. V6 is natively publically routeable
    - changes packet destination or source ip to public ip
    - this is why instance OS doesn't know public ip address

Bastion Host = Jumpbox
  - instance in subnet
  - manage routing, sometimes the only way into highly secure private VPCs

      



==============
Cloudformation
==============

Does cloud stuff based on config file
  - In YAML (maybe json?), Description field, if used, must directly follow AWSTemplateFormatVersion
  - Metadata field controls UI among other things
  - Parameters add options for user to select when using CFN UI
  - Mappings makes lookup tables
  - Conditions are a thing
  - Outputs are like, what is the resulting ec2 id and stuff, probably goes in a log somewhere

CFN templates create stacks (a template can start many stacks). Stacks make stuff (ec2 instance, etc.)
  - Deleting stack deletes the stuff
  - Template -> logical resource (stack) -> physical resource (stuff)


==============
Route53
==============

Globally resillient, no region

 
Register Domains
  - Has relationships with major domain registries, which manage top level domains as vested by IANA
  - Registration process:
    - First, check with top level organization if domain is available
    - Create zonefile (database for domain with DNS info)
    - Allocate nameservers (4 per zone) - this is a "hosted zone"
      - Tell top level org about these. They add nameserver records that communicate with hosted zone

Host Zones (managed nameservers) and general DNS flow
  - Can be public or private (linked to VPC)
  - stores records
    - nameserver records
      - "amazon.com" in the .com zone points to servers in the amazon.com zone
      - root zone similarly points to .com zone
    - A & AAAA records
      - in amazon.com zone, the host/name of www (subdomains may or may not be hosts) might map to an ip address.
        - CNAMES can point multiple domains to same host/name (NOT DIRECTLY TO IP)
      - A maps to ipv4, AAAA maps to ipv6
        - usually create both for client compatibility
    - Cname
      - see above
    - MX
      - routes emails to servers
        - A record for "mail"
        - MX record for "mail"
        - another MX record for "mail.stuff."
          - dot on right means it's fully qualified and can point outside zone
            - maybe a different mail service that the zone owner likes better
        - mail server sees to: stuff@gmail.com
          - queries gmail.com for MX records
          - resolves priority (lower # is higher prio) 
          - gets hostname, then has to query for IP
    - TXT
      - arbitrary, can be used to prove to google for instance that you own the domain you say you do
        - other patterns exist, but the key is only the owner can add stuff and that helps
  - TTL
    - Walking the above tree takes time, but you get an authoritative answer for what IP to hit
    - This answer can come with a TTL, which allows resolver server (like 8.8.8.8, for instance) to cache answer (now non-authoritative)
   


=========================
EBS - Elastic Block Store
=========================

EBS
  - Block network Storage attached to ec2 instance 
    - raw data, instance creates file system on top
    - lifecycle is independent from ec2 lifecycle, gets attached and detached
  - Can be encrypted via KMS
  - Provisioned per AZ
    - can get toward region resilience by backing snapshot up to S3
    - then why not replicate that to another region?
  - Generally one ec2 instance per EBS Volume
    - multi-attach can be managed but has to work to avoid corruption, overwrites, etc.
  - Billed GB/mo

Volume types:
  - GP (General Purpose)
    - types
      - GP2
        - 1GB to 16TB
        - Billing has credit bucket architecture
            - bucket has capacity of 15MM IO credits
              - gets that at start
                - nice for boots / initializations
              - no credit no IO
              - IO credit is 16KB
            - refills at Max(100,  3 * GB of storage) credits/sec
              - "baseline performance"
            - initial credits
            - can do up to 3000 IOps for <1TB volumes
              - after 1TB, max speed scales as fast as refill so doesn't deplete
              - caps at 16000 IOps for 5.33TB volume

      - GP3: base 3000 iops 125MiB/s, can pay for more
        - simpler, max 4x faster, cheaper
        - extra speed added manually
        - Best of both worlds between IO1 and GPT2
    - good for Virtual Desktops, medium sized DBs, low latency apps, boot volumes

  - IO (Provisioned IOPS)
    - types
      - IO1
        - up to 4x the top throughput as GP (same IOps)
        - speed independent of size
        - for low latency + consistency
      - IO1
        - speed independent of size
        - for low latency + consistency
      - Block Express
        - up to 4x the top throughput and IOps as IO1 & 2
    - Per instance limits apply when multiple volumes are attached to an instance
      - io1 and io2 block express are 260000IOps and 7500MiB/s
      - io2 is lower
      - instance type might have its own bottleneck

  - HDD
    - slow, cheap
    - can't boot from
    - 2 relevant types
      - st1: Throughput Optimized
        - good for sequential operations, not random access
          - big data, log processing
        - bucket system like gp2, base + burse
          - max 500MB/s
            - 40MB/s/tb base (refill rate)
            - 250MB/s/tb burst
      - sc1: cold
        - cheapest, for archives
        - max 250MB/s
          - 12MB/s/tb base
          - 80MB/s/tb burst

  - Encryption
    - OS doesn't see encryption
    - can't unencrypt a volume
    - create encrypted volume initially
      - KMS key is used
        - could be default EBS-managed key "aws/ebs"
        - could be customer-managed KMS key
          - KMS key creates encrypted DEK and KMS key is used to decrypt that
            - DEKs are per volume
            - decrypted DEK is stored in EC2 instance memory
            - now instance can decrypt volume
            - snapshots made from this volume (and volumes made from them) also use this DEK








=========================
EBS Snapshots
=========================

- Moves from EBS' availability zone resilience to S3 (region resilient)
- Incremental (think diffs)
- can be copied to other regions, can restore volumes
- restores are lazy, performance can suffer soon after restore
- FSR Fast Snapshot Restore costs extra, avoids laziness and you can have 50 per region
  - can just force the reads manually instead
- Billed GB/month
- snapshots only billed USED data on volume


=========================
IAM
=========================

Identities are IAM users, IAM groups (sort of), and IAM roles 

Identity policies are attached to Identities

ARN
  - unique identifers (unique within account)
    - most things unique across account + region
    - some things like IAM users unique within account
    - some things like s3 unique globally
  - slightly varied format
    - arn:aws:service:region:account-id:resource-id
    - arn:aws:service:region:account-id:resource-type/resource-id
    - arn:aws:service:region:account-id:resource-type:resource-id
  - "aws" is always the same for commercial aws (not government, etc)

Policy document (config) has statements with
  - action
    - "servicename:action", also can be specific action or list of them, including wildcards
  - resource
    - same as above, but instead of servicename:action, arn is used
  - effect 
    - allow or deny, conflicts are resolved in priority order
      - deny
      - allow
      - default is deny
  - optional Sid

Identity can have multiple policies
  - Someone can have 2 or more policies attached to them, and be in a group with another policy attached
    - same rules apply. If the group is explicitly denied, the user can't access it even if their personal roles specifically allow it.

Inline policies are not best practice
  - have to change it everywhere it's applied
  - can be good for exceptions when one identity needs special treatment
Managed Policy instead
  - create policy object and assign it to many things

IAM users are an identity used for long term AWS access (people, apps, service accounts)
  - if you can point to a named thing that needs access, it probably wants to have a user
  - Principal is the thing trying to authenticate as a user

5000 Users per account
  - for big companies, roles & identity federation can work
User can be in 10 groups

IAM Groups are containers for users
  - you cannot log into a group
  - like users, groups can have inline or managed policy attached
    - managed policies can be AWS-managed (built-in) or customer managed (custom)
  - no limit to how many users in group
  - no default "all users" group
  - no groups nested in groups
  - 300 groups limit
  - resource policy can't grant access to a group
    - they aren't true identities

IAM roles
  - temporary, a hat you put on for a short job
    - temp credentials are given via STS
  - roles have Trust policy and Permissions policy
    - Trust policy specifies who can assume role
      - can be users, services, can be used anonymously
    - Permissions policy specifies what permissions the role has
  - use cases
    - give aws services like lambda permission to stop ec2 instances or whatever
    - emergency role for break glass scenarios
    - existing identity provider (web identity federation)
      - no aws credentials stored in application
    - > 5000 staff
    - cross-account, give other account permissions
  - service-linked role
    - predefined role linked to an aws service
    - can't delete unless service is no longer using it
    - PassRole action permissions let you pass service-linked roles to other services
      - When an AWS service needs to assume a role, a user must have iam:PassRole permissions to pass that role to the service.


=========================
Organizations
=========================

Groups accounts
  - subgroups under Organizational Root are Organizational Units
    - can be nested

The account that makes the Organization is the management/master/payer account
  - can't be restricted
  - can invite accounts to join org
    - those accounts go from being standard accounts to member accounts
      - consolidated billing: Billing passes through to payer account
        - consolidates volume discounts

can create accounts directly in organizations
best practice is that only one account is logged into (maybe master acct, maybe not)
  - other accounts have roles this account can log into (role switching)
SCPs manage permissions for member accounts
  - can be attached to accounts, OUs, or Org root
  - inherit down
  - can't affect management account
  - can indirectly restrict account root user (not usually possible)
  - don't grant permissions, only defines what COULD be allowed
    - default SCP is FullAWSAccess
      - doesn't actually grant permissions, identity policies still needed


=========================
Cloudwatch
=========================

Public zone service
Regional
log = info + timestamp
many aws services are integrated by default
log stream is a sequence of log events from the same source
log group is a container of log streams
  - settings are set here, like retention settings
  - can generate metrics from log data
    - looks for patterns, increments metric, can trigger alarms


=========================
Cloudwatch
=========================


Has UI, CLI, and API interfaces (i imagine most services do)

Metrics
  - a metric is a time-ordered set of data points
    - Dimensions are used to identify
      - EC2 InstanceId could be a dimension, but so could InstanceType. Depends what they think someone might want to look at
      - can't add dimensions to native metrics, only custom
  - some things are automatically gathered (ec2 cpu usage)
  - some external things need Cloudwatch agent to add metrics

Logs
  - same same but different

Events
  - can perform actions based on metrics
  - can perform actions at a certain time
  - can for instance send an email based on an alarm

Namespace
  - custom for external data
  - "AWS/EC2" (for instance) for native things

logs API calls and aws activities as cloudtrail events
  - can be action taken by user, role, or service
  - stored 90 days by default, free
  - can be 1 of 3 kinds of events
    - management events
      - creating/terminating instances, create vpc
      - enabled by default
    - data events
      - objects added to s3, lambda invoked
      - not enabled by default because it's a lot!
    - insight events

can create trails for customization
  - logs events in a region
  - can be configured to log global service events
    - enabled by default
    - consolidated into us-east-1 specifically
  - can be all-region, which is basically just one in each region operating as one logical trail
    - so changes to configuration will apply to all regions and new regions are added automatically
  - can be stored in S3 as json
  - can be stored in Cloudwatch Logs as well
    - search through, use metric filters
  - you can make an organization trail using the org management account
    - for consolidation

There is a delay!
  - within 15min usually


=========================
AWS Control Tower
=========================

Orchestrates organizations and other services to set up a multi-account environment

Landing Zone
  - has home region
  - uses organizations, AWS Config, Cloudformation, etc. to work
  - like organization, has management account
  - sets up SSO, centralized logging
  - creates 2 OUs
    - Foundational "security"
      - creates 2 accounts
        - Audit
        - Log archive
          - users who need all logging info
    - Custom "sandbox"

Guard Rails
  - 3 types: Mandatory, Strongly Recommended, Elective
  - Function in 2 ways
    - Preventative: uses SCP to stop things from happening
    - Detective: uses AWS Config to detect when things happen
      - clear, in violation, or not enabled

Account Factory automates and standardizes new account creation
  - guard rails can be auto applied
  - account admin given to named user
  - configure network like ip ranges
  - accounds can be deleted
  - can be integrated into a business' SDLC


=========================
S3
=========================

Security
  - Private by default
  - Bucket policies
    - type of resource policy
    - how you give access to something to identities outside your account, since you can't attach identity policies to them
    - can grant access to anonymous principals
    - statements look like Identity policy's, with additional Principal field
    - can block by conditions like matching IP, uses MFA, more
  - Access Control List
    - Legacy, not recommended
    - Subresource (like object is to bucket)
      - can be attached to object or bucket
    - inflexible
      - no conditions
  - Block Public Access
    - apply only to anonymous principal
    - can block
      - everything
      - things granted by new ACLs
      - things granted by all ACLs
      - things granted by new resource/access point policies
      - things granted by all resource/access point policies

Static Website Hosting
  - need to point to an index and error page
  - if you want to use custom domain, bucket name needs to match
  - good for offloading image serving, etc. from ec2

Object versioning
  - controlled at bucket level
  - can be enabled and suspended, but cannot be disabled
    - suspending and deleting existing versions manually is kinda like disabling
  - does what you expect, stores multiple versions
  - version number is called "id", compared to the object "key" (like a filename)
  - can request specific version by ID
  - when we "delete" an object and don't give an ID, the current version becomes a delete marker (hides all versions)
    - can delete the delete marker
    - specifying id does actually delete the version
  - MFA Delete
    - must use MFA to delete versions and to suspend versioning (or enable it)

Perforance Optimization
  - Single PUT upload can be unreliable
    - needs to restart upload if connection drops
    - limited to 5GB, which is already ridiculous
  - Multipart Upload
    - minimum size is 100MB
    - 10000 max parts between 5mb and 5gb (except last remainder part)
    - each part can be restarted
    - faster
  - Transfer Acceleration
    - helps problem of public internet and ISPs tossing our upload all over the place before it gets to S3 instance we want
    - gets to closest AWS edge location via public internet, then tunneled via AWS-controlled network straight where it goes
    - off by default
    - bucket name must be dns compatible, with no periods
    - more benefits as distance increases between uploader and destination bucket

Encryption
  - Buckets are not encrypted. Objects are.
    - each object can use different encryption settings
  - encryption-in-transit is used, can't see inside the tunnel
    - some exceptions
  - client-side encryption-at-rest
    - just upload some encrypted data lol
    - no longer supported by itself, everything is encrypted
      - of course, they can't prevent you from doing both
  - server-side encryption-at-rest
    - unencrypted objects inside encrypted tunnel are encrypted by S3 when they get there
    - 3 types
      - SSE with S3-Manged keys (SSE-S3)
        - default
        - you provide plaintext object and S3 makes a key
        - no options, invisible
        - per-object key is further encrypted by master S3 key, plaintext discarded and cyphertext per-object key saved
        - can't stop S3 full admin from seeing all data
          - might be illegal re: medical records, etc.
          - no Role Separation
        - uses AES-256 encryption
      - SSE with keys in KMS (SSE-KMS)
        - uses KMS key that you can configure
        - this takes the place of the master S3 key above
        - have logging and auditing against key using Cloudtrail, etc.
      - SSE with Customer-provided keys (SSE-C)
        - you provide key and object
        - s3 encrypts object and hashes key + object, attaches hash to object
        - s3 discards key
        - managing key creation is good for high regulatory environments
  - Bucket Keys
    - without bucket keys, encryption process for SSE-KMS requires a lot of calls to KMS, multiple per upload
      - There is throttling with KMS keys, too, so it isn't just an efficiency thing
    - KMS key is used to create a time-limited bucket key, which can be used to generate DEKs per 
      - this works because the bucket key is re-deriveable from the KMS key.
      - Only the KMS key is needed to decrypt
      - Kinda seems the same as caching the KMS key in memory, but transparently with best practices
      - Fewer KMS Cloudtrail logs, and they will have the bucket ARN, not object ARN
  - Replicating plain text objects to a bucket with bucket key or default encryption can change the "ETAG"

Storage Classes
  - S3 Standard
    - Duplicated across at least 3 AZs
    - "11 9's" of durability, 1 lost object every 10,000 years
    - replication is checked for accuracy with CRCs
    - if an object is durably stored like this, HTTP/1.1 200 OK status.
    - pay for storage gb/m; transfer OUT per gb, not in; and price per 1000 requests
    - can be made public, no minimum duration or size
    - first byte latency in ms, fast 
    - balanced and good for frequently accessed data that is important and non-replaceable
  - S3 Standard-IA (Infrequent Access)
    - cheaper to store, but has retrieval fee
      - retrieval fee (basically an access fee) but not transfer fee could apply to data that never leaves AWS (accessing a log for instance)
    - minimum duration billed: 30 days
    - minimum billed object size: 128KB
    - good for few accesses of important, irreplaceable data
      - once per month?
  - S3 OneZone-IA (Infrequent Access)
    - Like above without replication
    - cheaper still
    - for data that can be replaced
    - Basically never fails but you wouldn't bet your business on it
  - S3 Glacier-Instant
    - Like Standard-IA, but cheaper to store, more expensive to access, and 90 day minimum
    - good for data accessed once per quarter
  - S3 Glacier-Flexible
    - Formerly S3 Glacier
    - 1/6 the cost of S3 standard
    - not immediately available
    - can't be made public, including static website hosting
    - retrieval process needs to be done in advance of accessing
      - temporarily puts them in S3 Standard-IA until accessed
      - 3 types of retrieval
        - Expedited: 1-5 min
        - Standard: 3-5 hours
        - Bulk: 5-12 hours
        - Faster = more expensive (duh)
    - 40kb min size
    - 90 day min duration
  - S3 Glacier-Deep Archive
    - cheaper yet
    - like above but 180 day min
    - Standard retrieval: 12hr
    - Bulk retrieval: 48hr
    - good for regulatory archives, stuff that shouldn't ever have to be retrieved but needs to be kept
    - good as secondary backup
  - S3 Intelligent Tiering
    - 5 levels kinda map to above
      - Frequent
      - Infrequent
      - Archive Instant
      - Archive 
      - Deep Archive 
    - monitors usage and moves from tier to tier intelligently
    - Archive and Deep Archive require application changes and has same "dethawing" process
      - They are optional
    - Tiers higher than Archive do not incur retrieval fees
    - monitoring and automation fee instead
    - good for long lived data with unpredictable usage spikes

Lifecycle Configuration
  - applied to bucket or group of objects (prefix, tags)
  - rules consist of actions
    - can't trigger based on access patterns
    - 2 types
      - Transition actions change storage class (say, move to IA after 30 days)
        - can transition from more expensive -> less  
          - Intelligent Tiering is between IA and OneZone IA
          - weird exception: can't transition from OneZone IA to Glacier Instant Retrieval
        - remember that transitioning small objects could incur cost bc min sizes
        - Transitioning from Standard requires object to be in Standard for 30 days
          - can still transition manually tho
          - weirdness: doing so to Standard-IA or OneZone-IA starts another 30 day timer until you can move down to glaciers
            - BUT ONLY IF YOU WANT TO DO IT WITH ONE RULE??
      - Expiration actions delete after a while

Replication
  - cross-region or same region replication
  - can replicate to bucket in other account
      - destination bucket needs to have bucket policy to trust source account
  - Replication configuration
    - destination bucket
    - IAM Role to use
    - all objects or a group of them (prefix, tags)
    - optionally change storage class
    - ownership (what account owns object)
      - might want to make this the destination account if cross-account replication
    - RTC (replication time control)
      - Speeds up to < 15min vs "best effort"
      - costs extra
  - not retroactive by default
    - can configure "batch replication" to replicate existing objects
  - must enable versioning on source and destination
  - one directional by default
  - can handle encrypted objects, though extra config needed for SSE-KMS.
  - source bucket needs permissions for objects
  - won't replicate changes to lifecycle config, tags by default, other system events
  - can't replicate Glacier Flexible or Glacier deep archive
  - no delete markers by default
  - use cases for same Region 
    - resilience where regulations means data stays in one region
    - could use to aggregate multiple buckets' logs
    - sync some prod and test data
  - use cases for cross Region 
    - global resilience
    - reduce latency

Presigned URL
  - to create, someone with permissions supplies credentials, bucket + object, expiration time
  - allows unauthenticated request for object upload/download
  - when given the URL, unauthenticated user operates on object as the person who generated it
    - permissions are NOT baked in at generation
  - this is how you would have a website with videos only for logged in users
    - web server asks s3 for presigned url to send to logged in user
    - otherwise, videos would have to be public
  - you can generate a useless presigned url by not having access to the specified object
    - permissions can change shortly after to make it useful
  - don't generate using an IAM role (temp role credentials could expire at unpredictable times and break the URL)

S3 Select and Glacier Select
  - use sql-like statements to retrieve parts of objects
    - data stored as JSON, CSV, others
  - because client side filtering isn't cost-effective for big objects

S3 Events
  - older feature, eventbridge might be better
  - Can be delivered to SNS topics, SQS queues, lambdas
  - can trigger on put, post, copy, CompleteMultiPartUpload, deletes, restores, some replication stuff
  - Event Notification Config
  - lambda, etc. needs resource policy to allow S3 principal to interact
    - weirdly no way to edit lambda resource policy via UI?

S3 Access logs
  - log accesses of source bucket to some other target bucket
  - best effort, few hours
  - happens through "log delivery group"
    - built-in service principal
  - target bucket needs ACL to accept from LDG
    - bucket policies can't specify built-in AWS service principals, go figure
  - newline-delimited records with space-delimited attributes
  - logging configuration sets prefix for logfile
  - need to manually delete

S3 Object Lock
  - Existing buckets need support to turn on
  - versioning required; individual versions are what is locked
  - means no overwrites or deletes of that version
  - 2 ways it manages retention, both or either (or none) can be active
    - Retention Period
      - specify days and years to retain.
      - 2 modes
        - Compliance Mode
          - retention period / lock itself cannot be reduced, even by account root user!
        - Governance Mode
          - similar, but you can get special permissions to remove the lock
    - Legal Hold
      - Not time-based, just on or off
      - For when a version is critical
  - Bucket default settings are possible in addition to individual vesion settings

- S3 Access Points
  - Intermediary between principal and bucket
    - simplifies access management
      - instead of monolithic bucket policies, different groups with different permissions can access same bucket
  - can be set to be accessed only directly from within vpc
    - needs vpc endpointin vpc
  - for some reason CLI command is important to remember: "aws s3control create-access-point"
  - access points have unique DNS addresses
  - common pattern is for bucket policy to allow everything from a given access point, then use access point policies to actually manage permissions


=========================
Key Management System
=========================

Regional and Public
  - multiregion keys are supported, off by default
Create, store, and manage keys for cryptography
  - both symmetric and asymmetric
Perform encrypt, decrypt, etc.
Keys never leave the service
Compliant with FIPS 140-2 level 2
KMS Keys
  - formerly CMKs
  - contain id, creation date, key (resource) policy, description, and state
  - container for actual key data
    - can be generated or imported
    - up to 4kb of key data
  - granular permissions: decrypt, create, encrypt, etc. are separate
  - Support rotation, on by default
    - rotation mostly invisible
    - old versions are kept so you can decrypt pre-rotation encrypted files
  - can alias, so that you can swap keys
Data Encryption Keys
  - DEKs use KMS Keys
    - linked to KMS key
  - work on > 4KB
  - KMS doesn't operate on DEKs, it gives it to you to work with
    - plaintext
    - cyphertext
      - can be used with linked KMS key to get plaintext DEK
    - workflow:
      - encrypt data using DEK plaintext
      - discard plaintext DEK
      - store encrypted DEK with data
Key Policy
  - every key has one
  - unlike other services, keys don't automatically trust account they're in
    - usually, you explicitly grant access to the account then use IAM to grant usage rights
    - in higher security situations, you might skip IAM and grant permissions directly in key policy
  - there's another thing called grants


=============================
Network Access Control List
=============================

Firewalls for VPC (see General -> Firewalls)

NACL
  - Stateless
  - Every subnet has one
    - connections within one subnet are not filtered
  - supports explicit allows and denies
    - Can block bad IPs for instance
  - supports ip and port ranges
  - NOT specificity based, but rather the first matching rule applies
    - * is the fallback, basically the same as using the last allowed rule number
      - "implicit deny", can't be changed 
  - managed at vpc level but can be attached (only) to subnets
  - default NACL allows all, customs are created with deny all 
  - can't reference logical resources; ips and ranges are all you get!


=============================
Security Groups
=============================

Security Group
  - Stateful
  - implicit deny, explicit allow
  - No explicit deny: needs NACL to override the useful explicit allow
  - attached to Elastic Network Interfaces
  - doesn't attach to instance, attaches to its network interface
  - supports logical resources including itself and other security groups
    - can set "source" to another security group's id
      - matches any source with the other sg attached (kinda weird)
      - can only reference SGs in same vpc
      - scales well
    - can set "source" to same security group's id
      - anything with sg attached can talk to each other or whatever


=============================
NAT Gateway
=============================

Network Address Translation
  - For giving private resource outgoing-only access to the internet
    - called "outgoing-only" but reponses are still allowed.
  - changes packets' src or dest ip
    - like how internet gateway does (static NAT) with public ip addresses being changed to internal instance IP
    - IP Masquerading: hiding CIDR blocks behind one IP
      - lets ipv4 addresses conflict without problem as long as the "mask" is unique
      - breaks incoming access
  Architecture
    - App tier vpc has private instances with private IPs, wants to update software
    - Web tier vpc has NAT Gateway
    - App tier route table points traffic to nat gateway instead of internet gateway
    - NAT Gateway records private source IP, other data in translation table
    - Changes source IP to its own, sends data to Internet Gateway and out
    - gets reponse, uses translation table to adjust response destination back to private IP
  - AZ Resilient (deploy into a region)
    - for region resilience, you need
      - NG in each AZ
      - route table pointing at it in each AZ
  - Managed service, scales to 45Gbps
  - billed for duration you have NAT Gateway and Data Volume
  - doesn't work with ipv6
    - use "Egress Only Gateway" instead

NAT Instance
  - instead, can do NAT with EC2 instance
  - more customizeable
  - can do port-forwarding
  - can use security groups instead of just NACLs


==============
EC2
==============

Architecture
  - Virtualization As a Service
  - AZ resilient
  - shared or dedicated hosts
    - "host" is actual hardware in AWS Datacenter
    - even shared hosts are isolated, but dedicated might be required by regulations
  - When instance is deployed to a subnet
    - it gets assigned a primary Elastic Network Interface in that subnet.
      - instances can have multiple network interfaces, even in other subnets, but must be same AZ
    - might connect to EBS
  - Instance (on shared host) changes host on stop/start or failure (not restart)
  - Network cards, EBS volumes, instance resources are all in AZ
    - can't connect ENI from one zone to instance in another for instance
  - usually instances of same type and generation will share a host

When to use
  - You have a traditional OS + App compute need
  - long-running
  - burst or steady state
    - many kinds of instances, one will suit your needs
  - monolithic stack
    - need certain database running, whatever
  - easy migration point to the cloud
  - good for disaster recovery
    - can recovery using AMIs, architect something with redundancy easily
  - sorta the default

Instance Types
  - Choosing an instance type affects certain things
    - resources (cpu, ram, etc)
    - bandwidth (storage and network)
    - system architecture (arm, x64), vendor (amd, intel)
    - additional features like a GPU
  - Categories
    - General Purpose
      - default, diverse workloads, balanced resource ratio (ram to cpu, say)
    - Compute Optimized
      - High CPU use: ML, Modeling, gaming, processing
    - Memory Optimized
      - High RAM use: large in-memory datasets, certain database workflows
    - Accelerated Computing
      - Niche uses: hardware GPU, field programmable gate arrays, etc.
    - Storage Optimized
      - Fast sequential and random IO
  - Example instance type name: R5dn.8xlarge
    - R: instance family
    - 5: 5th generation
    - dn: letters mean additional capabilities.
      - d is SSD storage (for some reason)
      - n is network storage
    - 8xlarge: size. like a 8X tee shirt, big!
    
Instance Store Volume
  - Block Storage connected to instance
  - local, physical
  - comes with price of instance
  - have to be attached at launch, not later
  - many types, some higher end types are D3 at 4.6GB/s, and I3 at 16 
    - IOps are crazy bigger

Networking
  - Every instance has 1 primary ENI in its subnet, can attach more secondaries (can be in other subnets)
    - secondaries can be attached/detached
      - if license is attached to mac address, this can be useful
    - multiple ENIs can mean multiple security groups
  - ENI owns things that you might think instance owns
    - MAC address
    - primary private IP (comes with dns name that resolves to it)
    - optional public IP (comes with dns name that resolves to private ip inside VPC, public ip outside)
    - more secondary IPs
      - can have one Elastic IP per private IP
        - will remove regular public IP
    - IPs change on host change.
  - security group affects all IPs on interface
  - can disable source/destination check
    - setting discards traffic not to or from one of ENI's IPs
    - needs to be off to function as NAT instance
  - OS DOESN'T KNOW ABOUT PUBLIC IPv4 ADDRESS

Amazon Machine Images
  - regional with unique ami ids in each region even for preset ones
    - can be copied between regions
  - can't be edited, just create another one
  - used to launch images
    - even used to launch preset images
    - can be AWS- or community- provided (ubuntu is community for instance)
    - can be commercial (like coming with a windows license)
    - when you make one, you can make it public, private, or shared with some accounts
      - these permissions are stored in AMI
  - consist of image + attached volumes
    - does not contain volue data, only references them
    - snapshots are created
      - used in AMI using block device mapping
        - maps /dev/xvda (block device), say, to EBS snapshot
  - at launch, snapshots are made into volumes and volumes are attached to correct block device
  - optimally, for consistency, you stop the instance before creating an AMI

Purchase Options (launch types)
  - On-demand
    - default, how you think it works
    - per-second billing while instance is running
      - associated resources may still be charged when instance is stopped, such as storage
    - could be unavailable if there is a major EC2 failure
  - Spot
    - spot price goes up and down, you set maximum price you'll pay
      - if spot price goes up past your max, your instance is terminated
    - good for interruptable operations
    - can come at a huge discount
  - Standard Reserved
    - commit to certain amount of usage for 1-3 years
    - can reserve and not use, wasting money
    - you can launch instances where you reserve space
      - either per AZ or per region
        - AZ reserves capacity, region does not
    - partial discount on instance that doesn't fit in your reservation
    - different ways to pay
      - no upfront
      - some upfront
      - all upfront
      - what you don't pay is billed as per second cost
  - Scheduled Reserved
    - you can schedule regular but not constant usage
    - doesn't support all instance types or regions
    - must schedule 1200 hours/year for 1 year at least
  - On Demand Capacity Reservation
    - in case of low availablility (like due to failure) AWS prioritizes reserved purchases, then on-demand
    - ODCR puts you in the reserved group for this purpose, without the commitment
    - book for a specific AZ
    - starts immediately, no scheduling for future
    - can't reserve what's not available
    - good for
      - important but short term projects
      - temporary scale-ups
  - Dedicated hosts
    - you pay for the physical host, can launch into at will
    - have to manage host capacity
    - why?
      - might have licensing tied to cpu
      - stopping and starting instance stays on same host
  - Dedicated instances
    - middle ground
    - hardware isn't shared, but you don't manage host
    - hourly fee for regions where this is enabled, plus extra fee for instances
    - why?
      - regulations
      - don't have the extra management overhead of dedicated hosts
  - Savings Plan
    - hourly commitment for 1 or 3 years
    - reserve general compute dollar amounts or specific EC2 plan
      - general is better if you use notjust ec2
    - savings plan also works for lambda and fargate
    - products have special savings plan rate
    - if commitment runs out, you just pay normal rate

Status Checks
  - 2 checks
    - system status: host working correctly
      - power/internet outage, hardware issues, etc
    - instance status: instance working correctly
      - corrupt file system, OS issues, networking misconfiguration (like setting public ipv4 address in OS) etc.
  - can set cloudwatch alarm
    - can perform actions like reboot
      - or "recover," which moves to another host and rebooting. Unlike usual host migration, private ip address is retained
        - only works on system status errors
        - only works on some instance types
        - doesn't work if there's an instance store

Termination Protection
  - turned on per-instance
  - adds a second permission needed to terminate an instance
  - functions as an approval process potentially

Shutdown behavior
  - lets you terminate an instance on shutdown, if desired

Instance Metadata
  - accessible at specific ip 169.254.169.254/latest/meta-data/(attribute, such as public-ipv4)
  - can give lots of info including
    - ipv4 public address
    - temp credentials from instance role
    - user-data (startup script)
    - no auth, not encrypted
      - can block via firewall

============================
Elastic Container Service
============================

managed container based compute service

Configuration
  - Container Definition
    - Tells ECS enough information about the single defined container
      - where your container is (point to a container registry)
      - what ports it will expose

  - Task Definition
    - represents whole application
      - can be made of multiple containers
    - stores
      - container definitions
      - what resources are used by task
      - networking mode
      - compatibility metadata
      - task role
        - temp credentials from role assumed by task
        - best practice for giving ECS containers AWS permissions

  - Service Definition
    - how do we want task to scale, including load balancing
    - how to handle failed tasks
    - can provide High Availability

Clusters
  - where containers run
  - you give ECS an image and it makes a container in a cluster
  - container is in a container registry
    - AWS has one, ECR
  - Tasks and services can be deployed into clusters
  - 2 modes
    - EC2 mode
      - EC2 Management Component
        - manages cluster, instance placement, scheduling
      - Auto Scaling Group manages horizontal scaling of instances
      - instances are provisioned
        - you pay for them even if you launch no containers
      - you have to worry about capacity of your cluster
      - task images are deployed as containers on these host instances
      - you can use spot pricing or reserved instances
    - Fargate mode
      - Serverless, you don't manage the EC2 instances
      - Containers run in Fargate Shared Infrastructure
      - tasks and services are given network interfaces in VPC

Use ECS vs just EC2 instances if you use containers
Use EC2 mode if you have a large workload and price is important (so you can use spot or reserved pricing)
Use Fargate mode if you have a large workload and low overhead is important
Use Fargate mode if you have a burst or periodic workload
  - with EC2 mode you always have a fleet of container hosts running
    - sort of? good tuning could scale down and be as efficient, but there's friction and it's overhead in any case


============================
Elastic Container Registry
============================

ECR
  - hosts container images
  - each registry can have many repositories with many images
  - images can have tags (unique within repository) 
  - Each AWS account has:
    - Public Registry
      - anyone can read, need permissions to write
    - Private Registry
      - need permissions to read or write
  - permissions integrated with IAM
  - Cross-region and cross-account replication available

Image security scanning
  - Basic
  - Enhanced
    - uses Inspector product
      - can scan OS and software packages for issues

Reporting
  - Near-real-time metrics through cloudwatch
  - API events logged to cloudtrail
  - Events generated for EventBridge


============================
Kubernetes
============================

Open-source, cloud-agnostic container orchestrator

Highly available cluster of resources organized to work as one unit

Architecture
  - Control Plane
    - manages scheduling, applications, scaling, and deployment
    - Components
      - API
        - nodes and other elements communicate with this
        - can be horizontally scaled for HA and performance
      - etcd
        - Highly Available key-value store
        - main backing store for cluster
        - kinda works as a queue of pods
      - kube-scheduler
        - identifies pods with no nodes
        - intelligently assigns node to pod
      - clound-controller-manager
        - interacts with AWS, Azure, other cloud platforms
      - kube-controller-manager
        - Node controller - monitors nodes and responds to outages 
        - Job controller - runs pods
        - Endpoint controller - runs pods
        - Service Account & Token controller - Account and API token creation
  - Cluster Nodes
    - VM or physical server
    - functions as worker in the cluster
    - Runs
      - container runtime like docker or containerd
      - kubelet to interact with control plane via kubernetes API
      - kube-proxy or k-proxy
        - coordinates networking with control pane
        - configures rules to allow communication with pods inside or services outside the cluster
          - nodes' OS-level networking
  - Pods
    - smallest unit of computing
    - description of some work to be done plus some metadata about where and how
    - can use multiple containers but one-container-per-pod is common
      - only have multiple if they are tightly coupled
    - ephemeral, should be stateless

  - Jobs
    - ad-hoc process
    - creates pods
    - can retry

  - Services
    - collection of pods, can be thought of as an application

  - Ingress
    - entrypoint for external actor to use service
    - Ingress -> Routing -> Service -> Pods
    - uses Ingress Controller
     - AWS would use a load balancer to control ingress

  - Persistent Storage
    - lives beyond life of an ephemeral pod


============================
Elastic Kubernetes Service
============================

AWS-managed Kubernetes
Can run different ways: on AWS, on AWS Outpost (on-premises mini-AWS), EKS Anywhere, or even open-source EKS distro
Control pane runs across multiple AZs
  - etcd also across multiple AZs
Integration with AWS products: ECR, ELB, IAM, VPC
Persistent storage can be EBS, EFS, FSx

Nodes
  - can be
    - Self-managed
      - EC2 nodes you're billed as usual for and manage
    - Managed Node Groups
      - also EC2 but provisioning is handled by AWS
    - Fargate pods
      - more hands-off
  - Have to consider node type for features like windows, GPU, Local Zones, etc

Networking
  - EKS Control Plane uses AWS-managed VPC 
    - ENIs injected into customer VPC to communicate with it
      - consumption is done through customer vpc via ingress configurations
      - also can use public endpoint













































